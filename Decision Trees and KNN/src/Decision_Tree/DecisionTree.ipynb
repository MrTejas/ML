{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score # Import train_test_split function\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the observed values (to be compared later)\n",
    "def save_observations(dt,flag):\n",
    "    file1 = open('./results.txt','a')\n",
    "\n",
    "    if(flag>=0):\n",
    "        file1.write(str(dt.formulation)+'\\t\\t\\t'\n",
    "                    +str(dt.criterion)+'\\t\\t\\t'\n",
    "                    +str(dt.max_depth)+'\\t\\t\\t'\n",
    "                    +str(dt.max_features)+'\\t\\t\\t'\n",
    "                    +str(dt.accuracy)+'\\t\\t\\t'\n",
    "                    +str(dt.f1_micro)+'\\t\\t\\t'\n",
    "                    +str(dt.f1_macro)+'\\t\\t\\t'\n",
    "                    +str(dt.precision)+'\\t\\t\\t'\n",
    "                    +str(dt.recall)+'\\n')\n",
    "    else:\n",
    "        file1.write('\\n')\n",
    "    file1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for powerset formulation\n",
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self,max_depth, max_features, criterion,formulation):\n",
    "        self.formulation = formulation\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.classtoid = {\n",
    "            'electronics':0,\n",
    "            'clothing':1,\n",
    "            'sports':2,\n",
    "            'furniture':3,\n",
    "            'beauty':4,\n",
    "            'food':5,\n",
    "            'home':6,\n",
    "            'books':7\n",
    "        }\n",
    "        \n",
    "    def loadDataset(self, file_name):\n",
    "        self.data = pd.read_csv(file_name)\n",
    "        \n",
    "    def generatePowerset(self):\n",
    "        # assigning 8 labels to the 8 simple classes\n",
    "        labels = [0,1,2,3,4,5,6,7]\n",
    "        n = len(labels)\n",
    "        \n",
    "        # creating a powerset (list of sets)\n",
    "        power_set = []\n",
    "        \n",
    "        for i in range(2**n):\n",
    "            subset = set()\n",
    "            for j in range(n):\n",
    "                if (i >> j) & 1:\n",
    "                    val = labels[j]\n",
    "                    subset.add(val)\n",
    "            power_set.append(subset)\n",
    "\n",
    "        # returns the list of sets - powerset (each set is a class)\n",
    "        return power_set\n",
    "    \n",
    "    def convertCategoricalToLabels(self,list1):\n",
    "        labels = set()\n",
    "        index = 0\n",
    "        mapping = {} \n",
    "        \n",
    "        for element in list1:\n",
    "            if(isinstance(element,set)):\n",
    "                labels.add(frozenset(element))\n",
    "            else:\n",
    "                labels.add(element)\n",
    "            \n",
    "        for element in labels:\n",
    "            # assigning integer labels to categorical data instances\n",
    "            mapping[element] = index\n",
    "            index = index + 1\n",
    "        # if(isinstance(list1[0],set)):\n",
    "        #     print('id1 = ',str(mapping[frozenset({'beauty','books'})]))\n",
    "        #     print('id2 = ',str(mapping[frozenset({'books','beauty'})]))\n",
    "            \n",
    "        # for each row entry of the column, append its id to a list\n",
    "        labels_list = []\n",
    "        for element in list1:\n",
    "            if(isinstance(element,set)):\n",
    "                labels_list.append(mapping[frozenset(element)])\n",
    "            else:\n",
    "                labels_list.append(mapping[element])\n",
    "\n",
    "        return labels_list\n",
    "    \n",
    "    def getMultioutput(self):\n",
    "        labels = self.data['labels']\n",
    "        labels_list = []\n",
    "        for label in labels:\n",
    "            words = label.split()\n",
    "            val = 0\n",
    "            for word in words:\n",
    "                i = self.classtoid[word]\n",
    "                val = val + 2**i\n",
    "            labels_list.append(val)\n",
    "        return labels_list\n",
    "    \n",
    "    def fit_powerset(self):\n",
    "        # creating a classifier object\n",
    "        clf = DecisionTreeClassifier(\n",
    "            criterion=self.criterion,\n",
    "            max_depth = self.max_depth,\n",
    "            max_features = self.max_features\n",
    "        )\n",
    "        \n",
    "        # converting X_train to numerical features (where required -> object only)\n",
    "        features = ['age', 'gender', 'income', 'education', 'married', 'children', 'city', 'occupation', 'purchase_amount', 'most bought item']\n",
    "        \n",
    "        for feature in features:    \n",
    "            col_data_type = self.data[feature].dtype\n",
    "            if(col_data_type=='object'):\n",
    "                converted = self.convertCategoricalToLabels(self.data[feature])\n",
    "                self.data[feature] = converted\n",
    "        \n",
    "        X_train = self.data[features]\n",
    "        \n",
    "        # now converting Y_train to numerical data (max 256 values)\n",
    "        ps = self.generatePowerset()\n",
    "        \n",
    "        column_index = 10 # column of Y_train\n",
    "        labels_list = []\n",
    "        for label in self.data.iloc[:,column_index]:\n",
    "            labels_list.append(label)\n",
    "        \n",
    "        Y_train = []\n",
    "        for label in labels_list:\n",
    "            words = label.split()\n",
    "            temp = set()\n",
    "            for word in words:    \n",
    "                index = self.classtoid[word]\n",
    "                temp.add(index)\n",
    "            Y_train.append(temp)\n",
    "            \n",
    "        Y_train = self.convertCategoricalToLabels(Y_train)\n",
    "        \n",
    "        \n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        \n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # now fitting the classifier with obtained X_train and Y_train and storing it\n",
    "        # print(X_train)\n",
    "        # print(Y_train)\n",
    "        clf.fit(self.X_train,self.Y_train)\n",
    "        self.classifier = clf\n",
    "        \n",
    "        \n",
    "    def fit_multioutput(self):\n",
    "        # creating a classifier object\n",
    "        clf = DecisionTreeClassifier(\n",
    "            criterion=self.criterion,\n",
    "            max_depth = self.max_depth,\n",
    "            max_features = self.max_features\n",
    "        )\n",
    "        \n",
    "        # converting X_train to numerical features (where required -> object only)\n",
    "        features = ['age', 'gender', 'income', 'education', 'married', 'children', 'city', 'occupation', 'purchase_amount', 'most bought item']\n",
    "        \n",
    "        for feature in features:    \n",
    "            col_data_type = self.data[feature].dtype\n",
    "            if(col_data_type=='object'):\n",
    "                converted = self.convertCategoricalToLabels(self.data[feature])\n",
    "                self.data[feature] = converted\n",
    "        \n",
    "        X_train = self.data[features]\n",
    "        \n",
    "        # now converting Y_train to numerical data (max 256 values)\n",
    "        Y_train = self.getMultioutput()\n",
    "        \n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        \n",
    "                \n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # now fitting the classifier with obtained X_train and Y_train and storing it\n",
    "        # print(X_train)\n",
    "        # print(Y_train)\n",
    "        clf.fit(self.X_train,self.Y_train)\n",
    "        self.classifier = clf\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        if(self.formulation=='powerset'):\n",
    "            self.fit_powerset()\n",
    "        else:\n",
    "            self.fit_multioutput()    \n",
    "    \n",
    "    def _predict(self):\n",
    "        self.Y_pred = self.classifier.predict(self.X_test)\n",
    "        # print(self.Y_test)\n",
    "        # print(Y_pred)\n",
    "        # bool_list = (self.Y_test==self.Y_pred)\n",
    "        # int_list = [1 if value else 0 for value in bool_list]\n",
    "        # print('accuracy = ',str(100*sum(int_list)/len(int_list)),'%')\n",
    "\n",
    "    def calc_performance(self):\n",
    "        self.accuracy = round(accuracy_score(self.Y_test, self.Y_pred),3)\n",
    "        self.f1_micro = round(f1_score(self.Y_test, self.Y_pred, average='micro',zero_division=1),3)\n",
    "        self.f1_macro = round(f1_score(self.Y_test, self.Y_pred, average='macro',zero_division=1),3)\n",
    "        self.precision = round(precision_score(self.Y_test, self.Y_pred, average='macro',zero_division=1),3)\n",
    "        self.recall = round(recall_score(self.Y_test, self.Y_pred, average='macro',zero_division=1),3)\n",
    "        self.confusion_mat = confusion_matrix(self.Y_test, self.Y_pred)\n",
    "        \n",
    "    def print_performance(self):\n",
    "        print(\"Accuracy:\", self.accuracy)\n",
    "        print(\"F1 Score (Micro):\", self.f1_micro)\n",
    "        print(\"F1 Score (Macro):\", self.f1_macro)\n",
    "        print(\"Precision:\", self.precision)\n",
    "        print(\"Recall:\", self.recall)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(self.confusion_mat)\n",
    "        \n",
    "    def getKfold(self,k):\n",
    "        \n",
    "        kth_fold_metric = cross_val_score(self.classifier, self.X, self.Y, cv=k, scoring='accuracy')\n",
    "\n",
    "        # Print the accuracy for each fold and the mean accuracy\n",
    "        for i, acc in enumerate(kth_fold_metric, 1):\n",
    "            print(f\"Fold {i}: Accuracy = {acc:.4f}\")\n",
    "\n",
    "        # Calculate and print the mean accuracy across all folds\n",
    "        avg_acc = np.mean(kth_fold_metric)\n",
    "        print(f\"Mean Accuracy across {k}-fold cross-validation: {avg_acc:.4f}\")\n",
    "        return avg_acc\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dt object\n",
    "dt1 = DecisionTree(5,5,'entropy','multioutput')\n",
    "dt1.loadDataset('./advertisement.csv')\n",
    "dt1.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.06\n",
      "F1 Score (Micro): 0.06\n",
      "F1 Score (Macro): 0.013\n",
      "Precision: 0.81\n",
      "Recall: 0.078\n",
      "Confusion Matrix:\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 3 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dt1._predict()\n",
    "dt1.calc_performance()\n",
    "dt1.print_performance()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tejas/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=24.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 0.0476\n",
      "Fold 2: Accuracy = 0.0476\n",
      "Fold 3: Accuracy = 0.0476\n",
      "Fold 4: Accuracy = 0.0238\n",
      "Fold 5: Accuracy = 0.0000\n",
      "Fold 6: Accuracy = 0.0714\n",
      "Fold 7: Accuracy = 0.0238\n",
      "Fold 8: Accuracy = 0.0476\n",
      "Fold 9: Accuracy = 0.0476\n",
      "Fold 10: Accuracy = 0.0238\n",
      "Fold 11: Accuracy = 0.0476\n",
      "Fold 12: Accuracy = 0.1190\n",
      "Fold 13: Accuracy = 0.0238\n",
      "Fold 14: Accuracy = 0.0714\n",
      "Fold 15: Accuracy = 0.0238\n",
      "Fold 16: Accuracy = 0.0238\n",
      "Fold 17: Accuracy = 0.0732\n",
      "Fold 18: Accuracy = 0.0488\n",
      "Fold 19: Accuracy = 0.0732\n",
      "Fold 20: Accuracy = 0.0000\n",
      "Fold 21: Accuracy = 0.0244\n",
      "Fold 22: Accuracy = 0.0244\n",
      "Fold 23: Accuracy = 0.0000\n",
      "Fold 24: Accuracy = 0.0244\n",
      "Mean Accuracy across 24-fold cross-validation: 0.0399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.039948703058459156"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1.getKfold(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "form = ['powerset','multioutput']\n",
    "crit = ['gini','entropy']\n",
    "max_dp = [3,5,10,20,30]\n",
    "max_feat = [3,5,7,9,10]\n",
    "\n",
    "for fr in form:\n",
    "    for c in crit:\n",
    "        for d in max_dp:\n",
    "            for f in max_feat:\n",
    "                dt1.formulation = fr\n",
    "                dt1.criterion = c\n",
    "                dt1.max_depth = d\n",
    "                dt1.max_features = f\n",
    "                dt1.fit()\n",
    "                dt1._predict()\n",
    "                dt1.calc_performance()\n",
    "                save_observations(dt1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
