{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a75d4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import mode\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import wandb\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba041167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialLogRegression:\n",
    "    \n",
    "    def __init__(self,learning_rate,num_epoch,num_classes, num_features):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epoch = num_epoch\n",
    "        self.loss = []\n",
    "        self.val_acc = []\n",
    "        self.val_prec = []\n",
    "        self.val_rec = []\n",
    "        self.val_f1 = []\n",
    "        self.train_acc = []\n",
    "        self.train_prec = []\n",
    "        self.train_rec = []\n",
    "        self.train_f1 = []\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def printParameters(self):\n",
    "        print('---------------------------------------')\n",
    "        print('Learning Rate  : ',self.learning_rate)\n",
    "        print('Num Epoches    : ',self.num_epoch)\n",
    "        print('Num Features   : ',self.num_features)\n",
    "        print('Num Classes    : ',self.num_classes)\n",
    "        print('---------------------------------------')\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        sm = np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "        return sm\n",
    "    \n",
    "    def Softmax(self,z):\n",
    "        exp = np.exp(z - np.max(z))\n",
    "        for i in range(len(z)):\n",
    "            exp[i]/=np.sum(exp[i])\n",
    "        return exp\n",
    "    \n",
    "    def oneHotEncoding(self, y):\n",
    "        y_encoded = np.zeros((len(y), self.num_classes))\n",
    "        y_encoded[np.arange(len(y)), y] = 1\n",
    "        return y_encoded\n",
    "    \n",
    "    def crossEntropyLoss(self,y_hat,y):\n",
    "        epsilon = 1e-10\n",
    "        y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "        prob = y_hat[np.arange(len(y)), y]\n",
    "        loss = -np.mean(np.log(prob))\n",
    "        return loss            \n",
    "    \n",
    "    def fit(self,X_train,y_train,X_val, y_val):\n",
    "        X = X_train\n",
    "        y = y_train\n",
    "        y_one_hot = self.oneHotEncoding(y)\n",
    "        \n",
    "        self.w = np.random.random((self.num_features,self.num_classes))\n",
    "        self.b = np.random.random(self.num_classes)\n",
    "        \n",
    "        # gradient descent on w and b\n",
    "        for epoch in range(self.num_epoch):\n",
    "            y_hat = self.Softmax(X@self.w + self.b)\n",
    "            del_w = np.dot(X.T,(y_hat-y_one_hot))\n",
    "            del_b = np.sum(y_hat-y_one_hot)\n",
    "            \n",
    "            self.w = self.w - self.learning_rate*del_w\n",
    "            self.b = self.b - self.learning_rate*del_b\n",
    "            \n",
    "            l = self.crossEntropyLoss(y_hat,y)\n",
    "            self.loss.append(l)\n",
    "#             wandb.log({\"loss\": l, \"epoch\": epoch}) \n",
    "            \n",
    "            \n",
    "            # storing validation accuracy and loss (and other metrics)\n",
    "            self.validate(X_val,y_val)\n",
    "            \n",
    "            # storing training accuracy and loss\n",
    "            y_hat = self.Softmax(X_train@self.w + self.b)\n",
    "            y_pred_train = np.argmax(y_hat,axis=1)\n",
    "            acc = accuracy_score(y_train, y_pred_train)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(y_train, y_pred_train, average='weighted', zero_division=1)\n",
    "            self.train_acc.append(acc)\n",
    "            self.train_prec.append(precision)\n",
    "            self.train_rec.append(recall)\n",
    "            self.train_f1.append(f1)\n",
    "            \n",
    "            \n",
    "#             wandb.log({\"Train Accuracy\": acc, \"epoch\": epoch}) \n",
    "#             wandb.log({\"accuracy\": acc, \"loss\": l})\n",
    "            # if (epoch % (self.num_epoch//10)) ==0:\n",
    "            #     print('Epoch : ',epoch,' Loss : ',l)\n",
    "            \n",
    "        \n",
    "    def validate(self,X_val, y_val):\n",
    "        y_hat = self.Softmax(X_val@self.w + self.b)\n",
    "        y_pred_val = np.argmax(y_hat,axis=1)\n",
    "        # uncomment this to print classification report after each validation\n",
    "        # print(classification_report(self.y_val, y_pred_val,zero_division=1))\n",
    "    \n",
    "        acc = accuracy_score(y_val, y_pred_val)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred_val, average='weighted', zero_division=1)\n",
    "\n",
    "        self.val_acc.append(acc)\n",
    "        self.val_prec.append(precision)\n",
    "        self.val_rec.append(recall)\n",
    "        self.val_f1.append(f1)\n",
    "\n",
    "    def predict(self,X_test,y_test, X_val,y_val):\n",
    "        y_hat = self.Softmax(X_test@self.w + self.b)\n",
    "        self.y_pred = np.argmax(y_hat,axis=1)\n",
    "        cr = classification_report(self.y_pred, y_test, zero_division=1)\n",
    "        # print(cr)\n",
    "        return self.y_pred,y_hat,cr\n",
    "#         print('Test Accuracy : ',round(accuracy_score(self.y_test, self.y_pred),3))\n",
    "#         precision, recall, f1, _ = precision_recall_fscore_support(self.y_test, self.y_pred, average='weighted', zero_division=1)\n",
    "#         print('Precision     : ',round(precision,3))\n",
    "#         print('Recall        : ',round(recall,3))\n",
    "#         print('f1 score      : ',round(f1,3))\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "485f8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor:\n",
    "    def __init__(self):\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "        self.confidence_metric = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Add a bias term to the input features\n",
    "        X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "\n",
    "        # Compute the coefficients using the normal equation\n",
    "        self.coefficients = np.linalg.inv(X_train_bias.T.dot(X_train_bias)).dot(X_train_bias.T).dot(y_train)\n",
    "\n",
    "        # Extract the intercept and coefficients\n",
    "        self.intercept = self.coefficients[0]\n",
    "        self.coefficients = self.coefficients[1:]\n",
    "\n",
    "    def predict(self, X,y):\n",
    "        # Add a bias term to the input features\n",
    "        X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        # Predict the target values\n",
    "        predictions = X_bias.dot(np.insert(self.coefficients, 0, self.intercept))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def compute_confidence_metric(self, X_val, y_val):\n",
    "        # Check for NaN values in input features or target values\n",
    "        if np.isnan(np.sum(X_val)) or np.isnan(np.sum(y_val)):\n",
    "            self.confidence_metric = np.nan\n",
    "        else:\n",
    "            # Use the validation set to compute a performance metric\n",
    "            predictions = self.predict(X_val,y_val)\n",
    "\n",
    "            # Calculate Mean Squared Error as a performance metric\n",
    "            mse = np.mean((y_val - predictions) ** 2)\n",
    "\n",
    "            # Confidence metric is the inverse of MSE (higher confidence for lower MSE)\n",
    "            self.confidence_metric = 1 / (1 + mse)\n",
    "\n",
    "        return self.confidence_metric\n",
    "\n",
    "    def plot_predictions_vs_actual(self,predictions, actual, title=\"Predictions vs Actual\"):\n",
    "        \"\"\"\n",
    "        Plot predictions and actual values on the same graph as a line plot.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions: Numpy array of predicted values\n",
    "        - actual: Numpy array of actual values\n",
    "        - title: Title of the plot (default is \"Predictions vs Actual\")\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(predictions, label='Predictions', marker='o')\n",
    "        plt.plot(actual, label='Actual', marker='x')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Values')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e189966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Classifier:\n",
    "    \n",
    "    def __init__(self,layer_sizes, batch_size, num_epoches, learning_rate, activation_function, optimization):\n",
    "        self.layer_sizes = layer_sizes # number of neurons in each layer\n",
    "        self.num_layers = len(self.layer_sizes) # total number of layers\n",
    "        self.batch_size = batch_size # only for mini-batch gradient descent\n",
    "        self.epoches = num_epoches\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_function = activation_function\n",
    "        self.optimization = optimization\n",
    "        \n",
    "        # initializing weights (needed once only)\n",
    "        self.initialize_weights()\n",
    "        \n",
    "        # validation loss and acc\n",
    "        self.validation_loss = []\n",
    "        self.validation_accuracy = []\n",
    "        \n",
    "        # training loss and acc\n",
    "        self.L = []\n",
    "        self.A = []\n",
    "        \n",
    "        # print('Initialized values successfully!')\n",
    "    \n",
    "    def printParameters(self):\n",
    "        print('-----------------------------------------------')\n",
    "        print('Number of Layers       : ',self.num_layers)\n",
    "        print('Layer sizes            : ',self.layer_sizes)\n",
    "        print('Batch size             : ',self.batch_size)\n",
    "        print('Activation Function    : ',self.activation_function)\n",
    "        print('Optimization Method    : ',self.optimization)\n",
    "        print('Learning Rate          : ',self.learning_rate)\n",
    "        print('Num Epoches            : ',self.epoches)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "    \n",
    "    # loads the dataframe into the class\n",
    "    def loadData(self,df):\n",
    "        self.num_features = df.shape[1]-1\n",
    "        self.num_classes = df.iloc[:, -1].nunique()\n",
    "        # print('Loaded Dataframe!')\n",
    "\n",
    "        \n",
    "    # returns the one-hot encoded version of a given vector\n",
    "    def oneHotEncoding(self, y):\n",
    "        y_encoded = np.zeros((len(y), self.num_classes))\n",
    "        y_encoded[np.arange(len(y)), y] = 1\n",
    "        return y_encoded\n",
    "    \n",
    "    def categorical(self,x):  \n",
    "        # Transform probabilities into categorical predictions row-wise, by simply taking the max probability\n",
    "        categorical = np.argmax(x,axis=1)\n",
    "        return categorical\n",
    "\n",
    "# defining activation functions and their derivatives\n",
    "#--------------------------------------------------------------------------------------------\n",
    "\n",
    "    # SIGMOID\n",
    "    def sigmoid(self,z):\n",
    "        val = 1/(1+np.exp(-z))\n",
    "        return val\n",
    "    def sigmoid_prime(self,h):\n",
    "        # Compute the derivative of sigmoid where h=sigmoid(x)\n",
    "        return h*(1-h)\n",
    "    \n",
    "    # ReLU \n",
    "    def ReLU(self, z):\n",
    "        ret = np.maximum(z, 0)\n",
    "        if ret.shape!=z.shape:\n",
    "            print('whaat??')\n",
    "        return ret\n",
    "    def ReLU_prime(self, h):\n",
    "        return np.where(h >= 0, 1, 0)\n",
    "    \n",
    "    # tanh\n",
    "    def hyperbolic_tan(self, z):\n",
    "#         val = (2/(np.exp(-2*z)+1)) -1\n",
    "        val = expit(2 * z) - 1\n",
    "\n",
    "        return val\n",
    "    def hyperbolic_tan_prime(self, h):\n",
    "        return 1-h**2\n",
    "    \n",
    "    # linear (can change slope and intercept)\n",
    "    def linear(self, z):\n",
    "        return z\n",
    "    def linear_prime(self,h):\n",
    "        return 1\n",
    "    \n",
    "#-----------------activation function for last layer (output) -------------------------------\n",
    "\n",
    "    def softmax(self,z):\n",
    "        ar = np.exp(z-np.max(z))\n",
    "        return ar/ar.sum(axis=1,keepdims=True)\n",
    "#         exp = np.exp(z - np.max(z))\n",
    "#         for i in range(len(z)):\n",
    "#             exp[i]/=np.sum(exp[i])\n",
    "#         return exp\n",
    "    \n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def activation(self,z):\n",
    "        if z.any()>=1e2:\n",
    "            print('z = ',z)\n",
    "        if self.activation_function=='sigmoid':\n",
    "            return self.sigmoid(z)\n",
    "        elif self.activation_function=='ReLU':\n",
    "            return self.ReLU(z)\n",
    "        elif self.activation_function=='tanh':\n",
    "            return self.hyperbolic_tan(z)\n",
    "        elif self.activation_function=='linear':\n",
    "            return self.linear(z)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation function\")\n",
    "\n",
    "    def activation_prime(self,h):\n",
    "        if self.activation_function=='sigmoid':\n",
    "            return self.sigmoid_prime(h)\n",
    "        elif self.activation_function=='ReLU':\n",
    "            return self.ReLU_prime(h)\n",
    "        elif self.activation_function=='tanh':\n",
    "            return self.hyperbolic_tan_prime(h)\n",
    "        elif self.activation_function=='linear':\n",
    "            return self.linear_prime(h)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation function\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        epsilon = 1e-7\n",
    "        y_hat = y_hat + epsilon\n",
    "        # adding small value to avoid underflow\n",
    "        # Compute the loss along the rows, averaging along the number of samples\n",
    "        return ((-np.log(y_hat))*y).mean()\n",
    "    \n",
    "    def accuracy(self, y_hat, y):  \n",
    "        # Compute the accuracy along the rows, averaging along the number of samples\n",
    "        return np.sum(y_hat==(y))/len(y_hat)\n",
    "\n",
    "\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        self.weights = []\n",
    "        #weights[i] is a m,n matrix giving weight connecting the i and i+1th layer\n",
    "        \n",
    "        min_weight = -1\n",
    "        max_weight = 1\n",
    "        for i in range(self.num_layers-1):\n",
    "            sz = [self.layer_sizes[i],self.layer_sizes[i+1]]\n",
    "            wt = np.random.uniform(min_weight,max_weight,size=sz)\n",
    "            self.weights.append(wt)\n",
    "        \n",
    "#         self.weights = np.asarray(self.weights)\n",
    "        \n",
    "    def initialize_layers(self,batch_size):\n",
    "        self.hidden_layers = [np.ones((batch_size,layer_size)) for layer_size in self.layer_sizes]\n",
    "        \n",
    "    # takes in a batch of data (num_samples x num_features) and applies feed-forward on it\n",
    "    def feedforward(self, batch):\n",
    "        current_layer = batch\n",
    "        self.hidden_layers[0] = batch\n",
    "        \n",
    "        # next_layer = activation(weights . current_leyer) and storing while feed-forward\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            current_layer = self.activation(np.dot(current_layer,weights))\n",
    "            self.hidden_layers[i+1] = current_layer\n",
    "            \n",
    "        # softmax of the last layer is the output layer\n",
    "        self.output_layer = self.softmax(self.hidden_layers[-1])\n",
    "        \n",
    "    # goes from the last layer to the 1st layer updating weights according to GD\n",
    "    def backpropogation(self, y):\n",
    "        y = self.oneHotEncoding(y)\n",
    "        # evaluating the last layer error : del\n",
    "        del_t = -(y - self.output_layer)*self.activation_prime(self.hidden_layers[-1])\n",
    "        \n",
    "        for i in range(1,self.num_layers):\n",
    "            # calculating the gradient of weights and applying gradient-descent\n",
    "            dJ_dW = np.dot(self.hidden_layers[-i-1].T,del_t)/self.batch_size\n",
    "            self.weights[-i] = self.weights[-i] - self.learning_rate * dJ_dW\n",
    "            # updating the error for the next layer \n",
    "            del_t = np.dot(del_t,self.weights[-i].T)*self.activation_prime(self.hidden_layers[-i-1])\n",
    "                    \n",
    "    def validate(self,X_val,y_val):\n",
    "        n,m = X_val.shape[0:2]\n",
    "        self.initialize_layers(batch_size=n)\n",
    "        self.feedforward(X_val)\n",
    "        val_loss = self.loss(self.output_layer,self.oneHotEncoding(y_val))\n",
    "        val_acc = self.accuracy(self.categorical(self.output_layer),y_val)\n",
    "        self.validation_loss.append(val_loss)\n",
    "        self.validation_accuracy.append(val_acc)\n",
    "        \n",
    "    def mini_batch_GD(self,X_train,y_train):\n",
    "            self.initialize_layers(self.batch_size)\n",
    "            loss_sum = 0\n",
    "            accuracy_sum = 0\n",
    "            \n",
    "            # calculating the total number of batches (acc to batch sizee)\n",
    "            num_batches = X_train.shape[0]/self.batch_size\n",
    "            ind = np.random.permutation(X_train.shape[0])\n",
    "            \n",
    "            # splitting the X_train, Y_train into batches\n",
    "            X_batches = np.array_split(X_train[ind], num_batches)\n",
    "            Y_batches = np.array_split(y_train[ind], num_batches)\n",
    "            data_batches = zip(X_batches,Y_batches)\n",
    "            \n",
    "            # performing feed-forward -> saving training loss and accuracy -> back-propogation\n",
    "            for data_x, data_y in data_batches:\n",
    "                self.feedforward(data_x)\n",
    "                loss_sum = loss_sum + self.loss(self.output_layer,self.oneHotEncoding(data_y))\n",
    "                accuracy_sum = accuracy_sum + self.accuracy(self.categorical(self.output_layer),data_y)\n",
    "                self.backpropogation(data_y)\n",
    "            \n",
    "            loss_train = loss_sum/num_batches\n",
    "            acc_train = accuracy_sum/num_batches\n",
    "            \n",
    "            return loss_train, acc_train\n",
    "        \n",
    "    def batch_GD(self,X_train,y_train):\n",
    "        self.batch_size = X_train.shape[0]\n",
    "        loss_train, acc_train = self.mini_batch_GD(X_train,y_train)\n",
    "        return loss_train, acc_train\n",
    "        \n",
    "    def SGD(self,X_train,y_train):\n",
    "        self.batch_size = 1\n",
    "        loss_train, acc_train = self.mini_batch_GD(X_train,y_train)\n",
    "        return loss_train, acc_train\n",
    "    \n",
    "    # general method for optimization (batch/mini-batch/SGD)\n",
    "    def optimize(self,X_train,y_train):\n",
    "        if self.optimization=='mini-batch':\n",
    "            return self.mini_batch_GD(X_train,y_train)\n",
    "        elif self.optimization=='batch':\n",
    "            return self.batch_GD(X_train,y_train)\n",
    "        elif self.optimization=='SGD':\n",
    "            return self.SGD(X_train,y_train)\n",
    "    \n",
    "    def fit(self,X_train,y_train,X_val,y_val):\n",
    "        for epoch in range(self.epoches):\n",
    "            # Gradient Descent\n",
    "            loss_train, acc_train = self.optimize(X_train,y_train)\n",
    "            \n",
    "            # calculating accuracy and loss for current epoch and saving them\n",
    "            self.L.append(loss_train)\n",
    "            self.A.append(acc_train)\n",
    "            \n",
    "            # testing the current model on validation set and saving the loss and accuracy\n",
    "            self.validate(X_val,y_val)\n",
    "            # if (epoch%(self.epoches//10)==0):\n",
    "                # print('Epoch : ',epoch+1,' loss : ',loss_train.round(3),' acc : ',acc_train.round(3))\n",
    "#             wandb.log({\"acc\": acc_train.round(3), \"loss\": loss_train.round(3)})\n",
    "            \n",
    "            \n",
    "    def predict(self,X_test,y_test,X_val,y_val):\n",
    "        self.feedforward(X_test)\n",
    "        cr = classification_report(self.categorical(self.output_layer),y_test,zero_division=1)\n",
    "        self.predictions = self.categorical(self.output_layer)\n",
    "        # print(self.predictions)\n",
    "        # print(self.output_layer)\n",
    "        return self.predictions,self.output_layer,cr\n",
    "        # return self.predictions,cr\n",
    "        # print('------------------------------------------------------\\n',cr,'\\n------------------------------------------------------\\n')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff3023b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Regressor:\n",
    "    \n",
    "    def __init__(self,layer_sizes, batch_size, num_epoches, learning_rate, activation_function, optimization):\n",
    "        self.layer_sizes = layer_sizes # number of neurons in each layer\n",
    "        self.num_layers = len(self.layer_sizes) # total number of layers\n",
    "        self.batch_size = batch_size # only for mini-batch gradient descent\n",
    "        self.epoches = num_epoches\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_function = activation_function\n",
    "        self.optimization = optimization\n",
    "        \n",
    "        # initializing weights (needed once only)\n",
    "        self.initialize_weights()\n",
    "        \n",
    "        # validation loss and acc\n",
    "        self.validation_loss = []\n",
    "        self.validation_accuracy = []\n",
    "        \n",
    "        # training loss and acc\n",
    "        self.L = []\n",
    "        self.A = []\n",
    "        \n",
    "        # print('Initialized values successfully!')\n",
    "    \n",
    "    def printParameters(self):\n",
    "        print('-----------------------------------------------')\n",
    "        print('Number of Layers          : ',self.num_layers)\n",
    "        print('Layer sizes               : ',self.layer_sizes)\n",
    "        print('Batch size(if mini-batch) : ',self.batch_size)\n",
    "        print('Activation Function       : ',self.activation_function)\n",
    "        print('Optimization Method       : ',self.optimization)\n",
    "        print('Learning Rate             : ',self.learning_rate)\n",
    "        print('Num Epoches               : ',self.epoches)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "    \n",
    "    # loads the dataframe into the class\n",
    "    def loadData(self,df):\n",
    "        self.num_features = df.shape[1]-1\n",
    "        self.num_classes = df.iloc[:, -1].nunique()\n",
    "        self.df = df\n",
    "        # print('Loaded Dataframe!')\n",
    "\n",
    "    # splits the dataframe loaded into train, validation and test sets\n",
    "    # def splitData(self,train_fraction,normalize):\n",
    "    #     X = df.iloc[:, :-1].values\n",
    "        \n",
    "    #     # Replace NaN values with the constant k\n",
    "    #     nan_mask = np.isnan(X)\n",
    "    #     k = 0\n",
    "    #     X[nan_mask] = k\n",
    "\n",
    "    #     if normalize:\n",
    "    #         mean = np.mean(X, axis=0)\n",
    "    #         std = np.std(X, axis=0)\n",
    "    #         X = (X - mean) / std\n",
    "    #     y = df.iloc[:, -1].values\n",
    "        \n",
    "\n",
    "    #     self.X_train, X_temp, self.y_train, y_temp = train_test_split(X, y, test_size=1-train_fraction, random_state=42)\n",
    "    #     self.X_val, self.X_test, self.y_val, self.y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    #     print('Splitted data into train, test and val sets!')\n",
    "        \n",
    "        \n",
    "    # returns the one-hot encoded version of a given vector\n",
    "    def oneHotEncoding(self, y):\n",
    "        return y\n",
    "        y_encoded = np.zeros((len(y), self.num_classes))\n",
    "        y_encoded[np.arange(len(y)), y] = 1\n",
    "        return y_encoded\n",
    "    \n",
    "    def categorical(self,x):  \n",
    "        return x\n",
    "    \n",
    "        # Transform probabilities into categorical predictions row-wise, by simply taking the max probability\n",
    "        categorical = np.argmax(x,axis=1)\n",
    "        return categorical\n",
    "\n",
    "# defining activation functions and their derivatives\n",
    "#--------------------------------------------------------------------------------------------\n",
    "\n",
    "    # SIGMOID\n",
    "    def sigmoid(self,z):\n",
    "        val = 1/(1+np.exp(-z))\n",
    "        return val\n",
    "    \n",
    "    def sigmoid_prime(self,h):\n",
    "        # Compute the derivative of sigmoid where h=sigmoid(x)\n",
    "        return h*(1-h)\n",
    "    \n",
    "    # ReLU \n",
    "    def ReLU(self, z):\n",
    "        ret = np.maximum(z, 0)\n",
    "        if ret.shape!=z.shape:\n",
    "            print('whaat??')\n",
    "        return ret\n",
    "    def ReLU_prime(self, h):\n",
    "        return np.where(h >= 0, 1, 0)\n",
    "    \n",
    "    # tanh\n",
    "    def hyperbolic_tan(self, z):\n",
    "        val = (2/(np.exp(-2*z)+1)) -1\n",
    "        val = expit(2 * z) - 1\n",
    "        return val\n",
    "    \n",
    "    def hyperbolic_tan_prime(self, h):\n",
    "        return 1-h**2\n",
    "    \n",
    "    # linear (can change slope and intercept)\n",
    "    def linear(self, z):\n",
    "        return z\n",
    "    def linear_prime(self,h):\n",
    "        return 1\n",
    "    \n",
    "#-----------------activation function for last layer (output) -------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def activation(self,z):\n",
    "        if self.activation_function=='sigmoid':\n",
    "            return self.sigmoid(z)\n",
    "        elif self.activation_function=='ReLU':\n",
    "            return self.ReLU(z)\n",
    "        elif self.activation_function=='tanh':\n",
    "            return self.hyperbolic_tan(z)\n",
    "        elif self.activation_function=='linear':\n",
    "            return self.linear(z)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation function\")\n",
    "\n",
    "    def activation_prime(self,h):\n",
    "        if self.activation_function=='sigmoid':\n",
    "            return self.sigmoid_prime(h)\n",
    "        elif self.activation_function=='ReLU':\n",
    "            return self.ReLU_prime(h)\n",
    "        elif self.activation_function=='tanh':\n",
    "            return self.hyperbolic_tan_prime(h)\n",
    "        elif self.activation_function=='linear':\n",
    "            return self.linear_prime(h)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation function\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        y = y.reshape(-1,1)\n",
    "        return np.sum((y_hat-y)**2)\n",
    "    \n",
    "    def accuracy(self, y_hat, y):  \n",
    "        nan_mask = np.isnan(y)\n",
    "        k = 0\n",
    "        y[nan_mask] = k\n",
    "        nan_mask = np.isnan(y_hat)\n",
    "        k = 0\n",
    "        y_hat[nan_mask] = k\n",
    "        \n",
    "\n",
    "        mse = mean_squared_error(y_hat, y)\n",
    "        return mse\n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights = []\n",
    "        #weights[i] is a m,n matrix giving weight connecting the i and i+1th layer\n",
    "        \n",
    "        mean_weight = 0  # Mean of the normal distribution\n",
    "        std_dev_weight = 1  # Standard deviation of the normal distribution\n",
    "        min_weight = -1\n",
    "        max_weight = 1\n",
    "        for i in range(self.num_layers-1):\n",
    "            sz = [self.layer_sizes[i],self.layer_sizes[i+1]]\n",
    "            wt = np.random.normal(mean_weight, std_dev_weight, size=sz)\n",
    "\n",
    "            self.weights.append(wt)\n",
    "        \n",
    "    def initialize_layers(self,batch_size):\n",
    "        self.hidden_layers = [np.ones((batch_size,layer_size)) for layer_size in layer_sizes]\n",
    "        \n",
    "    # takes in a batch of data (num_samples x num_features) and applies feed-forward on it\n",
    "    def feedforward(self, batch):\n",
    "        current_layer = batch\n",
    "        self.hidden_layers[0] = batch\n",
    "\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            current_layer = self.activation(np.dot(current_layer,weights))\n",
    "\n",
    "            self.hidden_layers[i+1] = current_layer\n",
    "            \n",
    "        # last layer is the output layer\n",
    "        self.output_layer = (self.hidden_layers[-1])\n",
    "        \n",
    "    # goes from the last layer to the 1st layer updating weights according to GD\n",
    "    def backpropogation(self, y):\n",
    "        # evaluating the last layer error : del\n",
    "        y = y.reshape(-1, 1)\n",
    "        del_t = -(y - self.output_layer)\n",
    "        for i in range(1,self.num_layers):\n",
    "            # calculating the gradient of weights and applying gradient-descent\n",
    "            dJ_dW = np.dot(self.hidden_layers[-i-1].T,del_t)/self.batch_size\n",
    "            self.weights[-i] = self.weights[-i] - self.learning_rate * dJ_dW\n",
    "            # updating the error for the next layer \n",
    "            del_t = np.dot(del_t,self.weights[-i].T)*self.activation_prime(self.hidden_layers[-i-1])\n",
    "                    \n",
    "    def validate(self,X_val,y_val):\n",
    "        n,m = X_val.shape[0:2]\n",
    "        self.initialize_layers(batch_size=n)\n",
    "        self.feedforward(X_val)\n",
    "        val_loss = self.loss(self.output_layer,y_val)\n",
    "        val_acc = self.accuracy(self.categorical(self.output_layer),y_val)\n",
    "        self.validation_loss.append(val_loss)\n",
    "        self.validation_accuracy.append(val_acc)\n",
    "        \n",
    "    def mini_batch_GD(self,X_train,y_train,X_val,y_val):\n",
    "            self.initialize_layers(self.batch_size)\n",
    "            loss_sum = 0\n",
    "            accuracy_sum = 0\n",
    "            \n",
    "            # calculating the total number of batches (acc to batch sizee)\n",
    "            num_batches = X_train.shape[0]/self.batch_size\n",
    "            ind = np.random.permutation(X_train.shape[0])\n",
    "            \n",
    "            \n",
    "            # splitting the X_train, Y_train into batches\n",
    "            X_batches = np.array_split(X_train[ind], num_batches)\n",
    "            Y_batches = np.array_split(y_train[ind], num_batches)\n",
    "            data_batches = zip(X_batches,Y_batches)\n",
    "            \n",
    "            # performing feed-forward -> saving training loss and accuracy -> back-propogation\n",
    "            for data_x, data_y in data_batches:\n",
    "                self.feedforward(data_x)\n",
    "\n",
    "                loss_sum = loss_sum + self.loss(self.output_layer,data_y)\n",
    "                accuracy_sum = accuracy_sum + self.accuracy(self.categorical(self.output_layer),data_y)\n",
    "                self.backpropogation(data_y)\n",
    "            \n",
    "            loss_train = loss_sum/num_batches\n",
    "            acc_train = accuracy_sum/num_batches\n",
    "            \n",
    "            return loss_train, acc_train\n",
    "        \n",
    "    def batch_GD(self,X_train,y_train,X_val,y_val):\n",
    "        self.batch_size = X_train.shape[0]\n",
    "        loss_train, acc_train = self.mini_batch_GD(X_train,y_train,X_val,y_val)\n",
    "        return loss_train, acc_train\n",
    "        \n",
    "    def SGD(self,X_train,y_train,X_val,y_val):\n",
    "        self.batch_size = 1\n",
    "        loss_train, acc_train = self.mini_batch_GD(X_train,y_train,X_val,y_val)\n",
    "        return loss_train, acc_train\n",
    "    \n",
    "    # general method for optimization (batch/mini-batch/SGD)\n",
    "    def optimize(self,X_train,y_train,X_val,y_val):\n",
    "        if self.optimization=='mini-batch':\n",
    "            return self.mini_batch_GD(X_train,y_train,X_val,y_val)\n",
    "        elif self.optimization=='batch':\n",
    "            return self.batch_GD(X_train,y_train,X_val,y_val)\n",
    "        elif self.optimization=='SGD':\n",
    "            return self.SGD(X_train,y_train,X_val,y_val)\n",
    "    \n",
    "    def fit(self,X_train,y_train,X_val,y_val):\n",
    "        for epoch in range(self.epoches):\n",
    "            # Gradient Descent\n",
    "            loss_train, acc_train = self.optimize(X_train,y_train,X_val,y_val)\n",
    "            \n",
    "            # calculating accuracy and loss for current epoch and saving them\n",
    "            self.L.append(loss_train)\n",
    "            self.A.append(acc_train)\n",
    "            \n",
    "            # testing the current model on validation set and saving the loss and accuracy\n",
    "            self.validate(X_val,y_val)\n",
    "            # if (epoch%(0.1*self.epoches)==0):\n",
    "            #     print('Epoch : ',epoch+1,' loss : ',loss_train.round(3),' acc : ',acc_train.round(3))\n",
    "\n",
    "\n",
    "    def compute_confidence_metric(self,X_val,y_val):\n",
    "        # y_val = y_val.reshape(-1,1)\n",
    "        self.feedforward(X_val)\n",
    "        predictions = self.output_layer\n",
    "        mse = np.mean((y_val - predictions) ** 2)\n",
    "        # Confidence metric is the inverse of MSE (higher confidence for lower MSE)\n",
    "        self.confidence_metric = 1 / (1 + mse)\n",
    "        return self.confidence_metric\n",
    "\n",
    "            \n",
    "    def predict(self,X_test,y_test):\n",
    "        # y_test = y_test.reshape(-1,1)\n",
    "        self.feedforward(X_test)\n",
    "        \n",
    "        # plt.plot(self.output_layer,label='Predicted')\n",
    "        # plt.plot(self.y_test,label='Ground Truth')\n",
    "        # plt.title('Predicted Values vs Ground Truth')\n",
    "        # plt.grid()\n",
    "        # plt.xlabel('Sample')\n",
    "        # plt.ylabel('Value')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "        \n",
    "        output_layer = self.output_layer\n",
    "        return output_layer\n",
    "        \n",
    "        # print('--------------------------------------------------------------------------------')\n",
    "        \n",
    "        # # Calculate Mean Squared Error (MSE)\n",
    "        # mse = mean_squared_error(y_test, output_layer)\n",
    "        # print(f'Mean Squared Error (MSE)                 : {mse:.2f}')\n",
    "\n",
    "        # # Calculate Root Mean Squared Error (RMSE)\n",
    "        # rmse = np.sqrt(mse)\n",
    "        # print(f'Root Mean Squared Error (RMSE)           : {rmse:.2f}')\n",
    "\n",
    "        # # Calculate R-squared (Coefficient of Determination)\n",
    "        # r_squared = r2_score(y_test, output_layer)\n",
    "        # print(f'R-squared (Coefficient of Determination) : {r_squared:.2f}')\n",
    "        \n",
    "        # print('--------------------------------------------------------------------------------')\n",
    "        \n",
    "    def plot_predictions_vs_actual(self,predictions, actual, title=\"Predictions vs Actual\"):\n",
    "        \"\"\"\n",
    "        Plot predictions and actual values on the same graph as a line plot.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions: Numpy array of predicted values\n",
    "        - actual: Numpy array of actual values\n",
    "        - title: Title of the plot (default is \"Predictions vs Actual\")\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(predictions, label='Predictions', marker='o')\n",
    "        plt.plot(actual, label='Actual', marker='x')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Values')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03f64175",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the Decision Tree Classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - **kwargs: Any additional arguments accepted by DecisionTreeClassifier\n",
    "        \"\"\"\n",
    "        self.model = DecisionTreeClassifier(**kwargs)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the Decision Tree Classifier to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training features\n",
    "        - y_train: Training labels\n",
    "        \"\"\"\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def validate(self,X_val, y_val):\n",
    "        predictions = self.model.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, predictions)\n",
    "        return accuracy\n",
    "\n",
    "    \n",
    "    def predict(self, X_test,y_test,X_val, y_val):\n",
    "        \"\"\"\n",
    "        Predict labels, soft assignments, and provide the classification report.\n",
    "\n",
    "        Parameters:\n",
    "        - X_test: Test features\n",
    "\n",
    "        Returns:\n",
    "        - predictions: Predicted labels\n",
    "        - soft_assignments: Soft assignments for each prediction\n",
    "        - report: Classification report\n",
    "        \"\"\"\n",
    "        value = self.validate(X_val, y_val)\n",
    "        predictions = self.model.predict(X_test)\n",
    "        soft_assignments = self.model.predict_proba(X_test)\n",
    "        report = classification_report(predictions,y_test,zero_division=1)\n",
    "\n",
    "        return predictions, value*soft_assignments, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5483d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeRegressor:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the Decision Tree Regressor.\n",
    "\n",
    "        Parameters:\n",
    "        - **kwargs: Any additional arguments accepted by DecisionTreeRegressor\n",
    "        \"\"\"\n",
    "        self.model = DecisionTreeRegressor(**kwargs)\n",
    "        self.confidence_metric = None\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Fit the Decision Tree Regressor to the training data and compute the confidence metric.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training features\n",
    "        - y_train: Training labels\n",
    "        - X_val: Validation features\n",
    "        - y_val: Validation labels\n",
    "        \"\"\"\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on validation data for computing confidence metric\n",
    "        predictions_val = self.model.predict(X_val)\n",
    "        mse_val = mean_squared_error(y_val, predictions_val)\n",
    "\n",
    "        # Confidence metric is the inverse of MSE on validation data (higher confidence for lower MSE)\n",
    "        self.confidence_metric = 1 / (1 + mse_val)\n",
    "\n",
    "    def compute_confidence_metric(self,X_val,y_val):\n",
    "        confidence_metric = self.confidence_metric\n",
    "        return confidence_metric\n",
    "\n",
    "\n",
    "    def predict(self, X_test,y_test):\n",
    "        \"\"\"\n",
    "        Predict values and provide a confidence metric.\n",
    "\n",
    "        Parameters:\n",
    "        - X_test: Test features\n",
    "\n",
    "        Returns:\n",
    "        - predictions: Predicted values\n",
    "        - confidence_metric: Confidence metric\n",
    "        \"\"\"\n",
    "        predictions = self.model.predict(X_test)\n",
    "        return predictions\n",
    "    \n",
    "    def plot_predictions_vs_actual(self,predictions, actual, title=\"Predictions vs Actual\"):\n",
    "        \"\"\"\n",
    "        Plot predictions and actual values on the same graph as a line plot.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions: Numpy array of predicted values\n",
    "        - actual: Numpy array of actual values\n",
    "        - title: Title of the plot (default is \"Predictions vs Actual\")\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(predictions, label='Predictions', marker='o')\n",
    "        plt.plot(actual, label='Actual', marker='x')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Values')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f851a3",
   "metadata": {},
   "source": [
    "## Stacking : Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7915f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacking:\n",
    "    def __init__(self, level1_estimator, method):\n",
    "        self.level1 = level1_estimator\n",
    "        self.method = method\n",
    "        \n",
    "    def generate_new_train_data_stacking(self,k,X_train,y_train):\n",
    "        \n",
    "        pred_m1 = []\n",
    "        pred_m2 = []\n",
    "        pred_m3 = []\n",
    "        \n",
    "        actual = []\n",
    "        \n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        # Store datasets for each fold\n",
    "        fold_datasets = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X_train):\n",
    "            X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "            y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "            fold_datasets.append((X_train_fold, y_train_fold, X_test_fold, y_test_fold))\n",
    "            \n",
    "            # 1st base model : DT-classifier\n",
    "            model = MyDecisionTreeClassifier()\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            predictions, soft_assignments, report = model.predict(X_test_fold,y_test_fold, X_test_fold, y_test_fold)\n",
    "            pred_m1.append(soft_assignments)\n",
    "            \n",
    "            \n",
    "            # 2nd Base model : MLP-classifier\n",
    "            layer_sizes = [11,5,5,6]\n",
    "            batch_size = 100\n",
    "            num_epoches = 100\n",
    "            learning_rate = 0.02\n",
    "            activation_function = 'ReLU'\n",
    "            optimization = 'mini-batch'\n",
    "            model = MLP_Classifier(layer_sizes=layer_sizes,\n",
    "                    batch_size=batch_size,\n",
    "                    num_epoches=num_epoches,\n",
    "                    learning_rate=learning_rate,\n",
    "                    activation_function=activation_function,\n",
    "                    optimization=optimization\n",
    "                    )\n",
    "\n",
    "            model.loadData(df)\n",
    "            model.fit(X_train_fold,y_train_fold,X_train_fold,y_train_fold)\n",
    "            predictions, soft_assignments ,cr = model.predict(X_test_fold,y_test_fold,X_test_fold,y_test_fold)\n",
    "            pred_m2.append(soft_assignments)\n",
    "            \n",
    "            # 3rd Base Model : Logistic Regression\n",
    "            learning_rate = 0.001\n",
    "            epoches = 100\n",
    "            mlr = MultinomialLogRegression(learning_rate,epoches,num_classes,num_features)\n",
    "            mlr.fit(X_train_fold,y_train_fold,X_train_fold,y_train_fold)\n",
    "            predictions,soft_assignments,cr = mlr.predict(X_test_fold,y_test_fold,X_test_fold,y_test_fold)\n",
    "            pred_m3.append(soft_assignments)\n",
    "            \n",
    "            actual.append(y_test_fold)\n",
    "\n",
    "        # print('pred_m3 : ',pred_m3)\n",
    "        pred_m1 = np.concatenate(pred_m1)\n",
    "        pred_m2 = np.concatenate(pred_m2)\n",
    "        pred_m3 = np.concatenate(pred_m3)\n",
    "        \n",
    "        actual = np.concatenate(actual)\n",
    "        # print(pred_m1)\n",
    "        # print(pred_m2)\n",
    "        # print(pred_m3)\n",
    "        \n",
    "        # print('fin acc1 : ',(np.sum(actual==pred_m1))/actual.shape[0])\n",
    "        # print('fin acc2 : ',(np.sum(actual==pred_m2))/actual.shape[0])\n",
    "        # print('fin acc3 : ',(np.sum(actual==pred_m3))/actual.shape[0])\n",
    "\n",
    "        X = np.column_stack((pred_m1, pred_m2, pred_m3))\n",
    "        return X, actual\n",
    "    \n",
    "    def generate_new_test_data_stacking(self,X_train, y_train, X_test,y_test):\n",
    "        # 1st base model : DT-classifier\n",
    "        model = MyDecisionTreeClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions1, soft_assignments1, report = model.predict(X_test,y_test, X_test, y_test)\n",
    "        \n",
    "        \n",
    "        # 2nd Base model : MLP-classifier\n",
    "        layer_sizes = [11,5,5,6]\n",
    "        batch_size = 100\n",
    "        num_epoches = 100\n",
    "        learning_rate = 0.02\n",
    "        activation_function = 'ReLU'\n",
    "        optimization = 'mini-batch'\n",
    "        model = MLP_Classifier(layer_sizes=layer_sizes,\n",
    "                batch_size=batch_size,\n",
    "                num_epoches=num_epoches,\n",
    "                learning_rate=learning_rate,\n",
    "                activation_function=activation_function,\n",
    "                optimization=optimization\n",
    "                )\n",
    "\n",
    "        model.loadData(df)\n",
    "        model.fit(X_train,y_train,X_train,y_train)\n",
    "        predictions2, soft_assignments2 ,cr = model.predict(X_test,y_test,X_test,y_test)\n",
    "\n",
    "        # 3rd Base Model : Logistic Regression\n",
    "        learning_rate = 0.001\n",
    "        epoches = 100\n",
    "        mlr = MultinomialLogRegression(learning_rate,epoches,6,11)\n",
    "        mlr.fit(X_train,y_train,X_train,y_train)\n",
    "        predictions3,soft_assignments3,cr = mlr.predict(X_test,y_test,X_test,y_test)\n",
    "\n",
    "        X = np.column_stack((soft_assignments1, soft_assignments2, soft_assignments3))\n",
    "        y = y_test\n",
    "        \n",
    "        return X,y\n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_new_train_data_blending(self,h,X,y):\n",
    "        X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=h, random_state=42)\n",
    "        \n",
    "        \n",
    "        # 1st base model : DT-classifier\n",
    "        model = MyDecisionTreeClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions1, soft_assignments1, report = model.predict(X_holdout,y_holdout, X_holdout, y_holdout)\n",
    "        \n",
    "        \n",
    "        # 2nd Base model : MLP-classifier\n",
    "        layer_sizes = [11,5,5,6]\n",
    "        batch_size = 100\n",
    "        num_epoches = 100\n",
    "        learning_rate = 0.02\n",
    "        activation_function = 'ReLU'\n",
    "        optimization = 'mini-batch'\n",
    "        model = MLP_Classifier(layer_sizes=layer_sizes,\n",
    "                batch_size=batch_size,\n",
    "                num_epoches=num_epoches,\n",
    "                learning_rate=learning_rate,\n",
    "                activation_function=activation_function,\n",
    "                optimization=optimization\n",
    "                )\n",
    "\n",
    "        model.loadData(df)\n",
    "        model.fit(X_train,y_train,X_train,y_train)\n",
    "        predictions2, soft_assignments2 ,cr = model.predict(X_holdout,y_holdout,X_holdout,y_holdout)\n",
    "\n",
    "        # 3rd Base Model : Logistic Regression\n",
    "        learning_rate = 0.001\n",
    "        epoches = 100\n",
    "        mlr = MultinomialLogRegression(learning_rate,epoches,num_classes,num_features)\n",
    "        mlr.fit(X_train,y_train,X_train,y_train)\n",
    "        predictions3,soft_assignments3,cr = mlr.predict(X_holdout,y_holdout,X_holdout,y_holdout)\n",
    "        \n",
    "        X_new = np.column_stack((soft_assignments1, soft_assignments2, soft_assignments3, X_holdout))\n",
    "        y_new = y_holdout\n",
    "        \n",
    "        return X_new, y_new\n",
    "        \n",
    "    def generate_new_test_data_blending(self,X_train, y_train, X_test,y_test):\n",
    "        \n",
    "        # 1st Base model : DT-Classifier\n",
    "        model = MyDecisionTreeClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions1, soft_assignments1, report = model.predict(X_test,y_test, X_test, y_test)\n",
    "        \n",
    "        \n",
    "        # 2nd Base model : MLP-classifier\n",
    "        layer_sizes = [11,5,5,6]\n",
    "        batch_size = 100\n",
    "        num_epoches = 100\n",
    "        learning_rate = 0.02\n",
    "        activation_function = 'ReLU'\n",
    "        optimization = 'mini-batch'\n",
    "        model = MLP_Classifier(layer_sizes=layer_sizes,\n",
    "                batch_size=batch_size,\n",
    "                num_epoches=num_epoches,\n",
    "                learning_rate=learning_rate,\n",
    "                activation_function=activation_function,\n",
    "                optimization=optimization\n",
    "                )\n",
    "\n",
    "        model.loadData(df)\n",
    "        model.fit(X_train,y_train,X_train,y_train)\n",
    "        predictions2, soft_assignments2 ,cr = model.predict(X_test,y_test,X_test,y_test)\n",
    "\n",
    "        # 3rd Base Model : Logistic Regression\n",
    "        learning_rate = 0.001\n",
    "        epoches = 100\n",
    "        mlr = MultinomialLogRegression(learning_rate,epoches,num_classes,num_features)\n",
    "        mlr.fit(X_train,y_train,X_train,y_train)\n",
    "        predictions3,soft_assignments3,cr = mlr.predict(X_test,y_test,X_test,y_test)\n",
    "        \n",
    "        X_new = np.column_stack((soft_assignments1, soft_assignments2, soft_assignments3, X_test))\n",
    "        y_new = y_test\n",
    "        \n",
    "        return X_new, y_new\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def fit_level1_estimator(self,k,h,X_train,y_train):\n",
    "        if self.method=='stacking':\n",
    "            X,y = self.generate_new_train_data_stacking(k,X_train,y_train)\n",
    "        else:\n",
    "            X,y = self.generate_new_train_data_blending(h,X_train,y_train)\n",
    "        \n",
    "        if self.level1=='MultinomialLogRegression':\n",
    "            learning_rate = 0.001\n",
    "            epoches = 100\n",
    "            self.model = MultinomialLogRegression(learning_rate,epoches,6,X.shape[1])\n",
    "            self.model.fit(X,y,X,y)\n",
    "        elif self.level1=='DecisionTreeClassifier':\n",
    "            self.model = MyDecisionTreeClassifier()\n",
    "            self.model.fit(X, y)\n",
    "        else:\n",
    "            print('No such level1 model!')\n",
    "\n",
    "    def predict(self,X_train,y_train,X_test,y_test):\n",
    "        if self.method=='stacking':\n",
    "            X,y = self.generate_new_test_data_stacking(X_train,y_train,X_test,y_test)\n",
    "        else:\n",
    "            X,y = self.generate_new_test_data_blending(X_train, y_train, X_test, y_test)\n",
    "            \n",
    "        # print(X)\n",
    "        if self.level1=='MultinomialLogRegression':\n",
    "            self.predictions,soft_assignments,cr = self.model.predict(X,y,X,y)\n",
    "        elif self.level1=='DecisionTreeClassifier':\n",
    "            self.predictions, soft_assignments, report = self.model.predict(X,y,X,y)\n",
    "        else:\n",
    "            print('No such level1 model!')\n",
    "\n",
    "        predictions = self.predictions\n",
    "        # print(predictions)\n",
    "        # print(y)\n",
    "        \n",
    "        self.accuracy = (np.sum(predictions==y)/y.shape[0]) \n",
    "        return predictions, y\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loading the data\n",
    "file_path = './WineQT.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop(df.columns[-1], axis=1)\n",
    "labels = df.columns.tolist()\n",
    "\n",
    "num_features = df.shape[1]-1\n",
    "num_classes = df.iloc[:, -1].nunique()\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - mean) / std\n",
    "\n",
    "\n",
    "f=0.8\n",
    "\n",
    "y = df.iloc[:, -1].values\n",
    "y = y - 3\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1-f, random_state=42)\n",
    "# then splitting the remaining data into val and test (50% each)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "Change Parameters to observe results accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stk = Stacking('MultinomialLogRegression','stacking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stk.fit_level1_estimator(3,0.1,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, gt = stk.predict(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.6434782608695652\n"
     ]
    }
   ],
   "source": [
    "acc_ensemble = stk.accuracy\n",
    "print('Accuracy : ',acc_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtejas591995\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081227-t5axogzl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/t5axogzl' target=\"_blank\">elated-wildflower-1</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/t5axogzl' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/t5axogzl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:t5axogzl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.52174</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-wildflower-1</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/t5axogzl' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/t5axogzl</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081227-t5axogzl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:t5axogzl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081241-caq3uwk8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/caq3uwk8' target=\"_blank\">denim-shadow-2</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/caq3uwk8' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/caq3uwk8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:caq3uwk8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.46957</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">denim-shadow-2</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/caq3uwk8' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/caq3uwk8</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081241-caq3uwk8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:caq3uwk8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081255-bfnwpl3i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/bfnwpl3i' target=\"_blank\">devoted-jazz-3</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/bfnwpl3i' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/bfnwpl3i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bfnwpl3i) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.44348</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-jazz-3</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/bfnwpl3i' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/bfnwpl3i</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081255-bfnwpl3i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bfnwpl3i). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081305-nays4b9f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/nays4b9f' target=\"_blank\">faithful-smoke-4</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/nays4b9f' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/nays4b9f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nays4b9f) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.51304</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">faithful-smoke-4</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/nays4b9f' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/nays4b9f</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081305-nays4b9f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nays4b9f). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081314-cnmnwa6g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/cnmnwa6g' target=\"_blank\">avid-dew-5</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/cnmnwa6g' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/cnmnwa6g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cnmnwa6g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.50435</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">avid-dew-5</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/cnmnwa6g' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/cnmnwa6g</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081314-cnmnwa6g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cnmnwa6g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081323-7jqkktfv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/7jqkktfv' target=\"_blank\">blooming-sound-6</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/7jqkktfv' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/7jqkktfv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7jqkktfv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.58261</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-sound-6</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/7jqkktfv' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/7jqkktfv</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081323-7jqkktfv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7jqkktfv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081331-mdlo1uov</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/mdlo1uov' target=\"_blank\">clean-lake-7</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/mdlo1uov' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/mdlo1uov</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:mdlo1uov) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.48696</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-lake-7</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/mdlo1uov' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/mdlo1uov</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081331-mdlo1uov/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:mdlo1uov). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081339-iiawbxsz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/iiawbxsz' target=\"_blank\">fine-serenity-8</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/iiawbxsz' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/iiawbxsz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:iiawbxsz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.64348</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine-serenity-8</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/iiawbxsz' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/iiawbxsz</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081339-iiawbxsz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:iiawbxsz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081355-u45ktcer</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/u45ktcer' target=\"_blank\">balmy-armadillo-9</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/u45ktcer' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/u45ktcer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:u45ktcer) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.61739</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-armadillo-9</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/u45ktcer' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/u45ktcer</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081355-u45ktcer/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:u45ktcer). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081410-izuqjlhl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/izuqjlhl' target=\"_blank\">scarlet-paper-10</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/izuqjlhl' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/izuqjlhl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:izuqjlhl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.67826</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scarlet-paper-10</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/izuqjlhl' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/izuqjlhl</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081410-izuqjlhl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:izuqjlhl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081421-to5etcg8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/to5etcg8' target=\"_blank\">devoted-valley-11</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/to5etcg8' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/to5etcg8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:to5etcg8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.62609</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-valley-11</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/to5etcg8' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/to5etcg8</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081421-to5etcg8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:to5etcg8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081430-kpc9s5qg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/kpc9s5qg' target=\"_blank\">fast-river-12</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/kpc9s5qg' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/kpc9s5qg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kpc9s5qg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.56522</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fast-river-12</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/kpc9s5qg' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/kpc9s5qg</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081430-kpc9s5qg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:kpc9s5qg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081439-t413vc1s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/t413vc1s' target=\"_blank\">upbeat-sky-13</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/t413vc1s' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/t413vc1s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:t413vc1s) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.6</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-sky-13</strong> at: <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/t413vc1s' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/t413vc1s</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_081439-t413vc1s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:t413vc1s). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tejas/Documents/SMAI/assignment-4-MrTejas/q2/wandb/run-20231112_081448-h9veqyj2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/h9veqyj2' target=\"_blank\">clean-paper-14</a></strong> to <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tejas591995/Stacking%20Classifier/runs/h9veqyj2' target=\"_blank\">https://wandb.ai/tejas591995/Stacking%20Classifier/runs/h9veqyj2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level1_list = ['DecisionTreeClassifier','MultinomialLogRegression']\n",
    "methods_list = ['stacking','blending']\n",
    "h_vals = [0.1,0.2,0.3,0.4]\n",
    "k_vals = [7,5,3]\n",
    "\n",
    "for level1_model in level1_list:\n",
    "    for method in methods_list:\n",
    "        if method == 'stacking':\n",
    "            for k in k_vals:\n",
    "                h = 0.1\n",
    "                wandb.init(\n",
    "                    # set the wandb project where this run will be logged\n",
    "                    project=\"Stacking Classifier\",\n",
    "\n",
    "                    # track hyperparameters and run metadata\n",
    "                    config={\n",
    "                    'level1_model' : level1_model,\n",
    "                    'method' : method,\n",
    "                    'number of folds' : k,\n",
    "                    'holdout fraction' : None\n",
    "                    }\n",
    "                )                    \n",
    "                stk = Stacking(level1_model,method)\n",
    "                stk.fit_level1_estimator(k,h,X_train,y_train)\n",
    "                predictions, ground_truth = stk.predict(X_train,y_train,X_test,y_test)\n",
    "                acc = stk.accuracy\n",
    "                wandb.log({\"Accuracy\": acc})\n",
    "        else:\n",
    "            for h in h_vals:\n",
    "                k = 5\n",
    "                wandb.init(\n",
    "                    # set the wandb project where this run will be logged\n",
    "                    project=\"Stacking Classifier\",\n",
    "\n",
    "                    # track hyperparameters and run metadata\n",
    "                    config={\n",
    "                    'level1_model' : level1_model,\n",
    "                    'method' : method,\n",
    "                    'number of folds' : None,\n",
    "                    'holdout fraction' : h\n",
    "                    }\n",
    "                )                    \n",
    "                stk = Stacking(level1_model,method)\n",
    "                stk.fit_level1_estimator(k,h,X_train,y_train)\n",
    "                predictions, ground_truth = stk.predict(X_train,y_train,X_test,y_test)\n",
    "                acc = stk.accuracy\n",
    "                wandb.log({\"Accuracy\": acc})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The best performing model has the following hyper-parameters :-\n",
    "- method : stacking\n",
    "- number of folds : 3\n",
    "- Accuracy : 0.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1st base model : DT-classifier\n",
    "model = MyDecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "predictions1, soft_assignments1, report = model.predict(X_test,y_test, X_test, y_test)\n",
    "\n",
    "# 2nd Base model : MLP-classifier\n",
    "layer_sizes = [11,5,5,6]\n",
    "batch_size = 100\n",
    "num_epoches = 100\n",
    "learning_rate = 0.02\n",
    "activation_function = 'ReLU'\n",
    "optimization = 'mini-batch'\n",
    "model = MLP_Classifier(layer_sizes=layer_sizes,\n",
    "        batch_size=batch_size,\n",
    "        num_epoches=num_epoches,\n",
    "        learning_rate=learning_rate,\n",
    "        activation_function=activation_function,\n",
    "        optimization=optimization\n",
    "        )\n",
    "\n",
    "model.loadData(df)\n",
    "model.fit(X_train,y_train,X_train,y_train)\n",
    "predictions2, soft_assignments2 ,cr = model.predict(X_test,y_test,X_test,y_test)\n",
    "\n",
    "# 3rd Base Model : Logistic Regression\n",
    "learning_rate = 0.001\n",
    "epoches = 100\n",
    "mlr = MultinomialLogRegression(learning_rate,epoches,num_classes,num_features)\n",
    "mlr.fit(X_train,y_train,X_train,y_train)\n",
    "predictions3,soft_assignments3,cr = mlr.predict(X_test,y_test,X_test,y_test)\n",
    "\n",
    "acc1 = accuracy_score(y_pred=predictions1,y_true=y_test)\n",
    "acc2 = accuracy_score(y_pred=predictions2,y_true=y_test)\n",
    "acc3 = accuracy_score(y_pred=predictions3,y_true=y_test)\n",
    "\n",
    "values = [acc1, acc2, acc3, acc_ensemble]\n",
    "labels = ['Decision Tree', 'MLP', 'MLR', 'Stacking']\n",
    "\n",
    "# Create a bar chart\n",
    "plt.bar(labels, values)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracies')\n",
    "plt.grid()\n",
    "plt.title('Accuracies of ensemble vs base learners')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies\n",
      "DecisionTreeClassifier :  0.5589519650655022\n",
      "Multi-Layer-Perceptron :  0.4192139737991266\n",
      "Logistic Regression    :  0.5109170305676856\n",
      "Stacking Ensemble    :  0.6434782608695652\n"
     ]
    }
   ],
   "source": [
    "print('Accuracies')\n",
    "print('DecisionTreeClassifier : ',acc1)\n",
    "print('Multi-Layer-Perceptron : ',acc2)\n",
    "print('Logistic Regression    : ',acc3)\n",
    "print('Stacking Ensemble    : ',acc_ensemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
