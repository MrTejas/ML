{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a75d4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import mode\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba041167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialLogRegression:\n",
    "    \n",
    "    def __init__(self,learning_rate,num_epoch,num_classes, num_features):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epoch = num_epoch\n",
    "        self.loss = []\n",
    "        self.val_acc = []\n",
    "        self.val_prec = []\n",
    "        self.val_rec = []\n",
    "        self.val_f1 = []\n",
    "        self.train_acc = []\n",
    "        self.train_prec = []\n",
    "        self.train_rec = []\n",
    "        self.train_f1 = []\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def printParameters(self):\n",
    "        print('---------------------------------------')\n",
    "        print('Learning Rate  : ',self.learning_rate)\n",
    "        print('Num Epoches    : ',self.num_epoch)\n",
    "        print('Num Features   : ',self.num_features)\n",
    "        print('Num Classes    : ',self.num_classes)\n",
    "        print('---------------------------------------')\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        sm = np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "        return sm\n",
    "    \n",
    "    def Softmax(self,z):\n",
    "        exp = np.exp(z - np.max(z))\n",
    "        for i in range(len(z)):\n",
    "            exp[i]/=np.sum(exp[i])\n",
    "        return exp\n",
    "    \n",
    "    def oneHotEncoding(self, y):\n",
    "        y_encoded = np.zeros((len(y), self.num_classes))\n",
    "        y_encoded[np.arange(len(y)), y] = 1\n",
    "        return y_encoded\n",
    "    \n",
    "    def crossEntropyLoss(self,y_hat,y):\n",
    "        epsilon = 1e-10\n",
    "        y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "        prob = y_hat[np.arange(len(y)), y]\n",
    "        loss = -np.mean(np.log(prob))\n",
    "        return loss            \n",
    "    \n",
    "    def fit(self,X_train,y_train,X_val, y_val):\n",
    "        X = X_train\n",
    "        y = y_train\n",
    "        y_one_hot = self.oneHotEncoding(y)\n",
    "        \n",
    "        self.w = np.random.random((self.num_features,self.num_classes))\n",
    "        self.b = np.random.random(self.num_classes)\n",
    "        \n",
    "        # gradient descent on w and b\n",
    "        for epoch in range(self.num_epoch):\n",
    "            y_hat = self.Softmax(X@self.w + self.b)\n",
    "            del_w = np.dot(X.T,(y_hat-y_one_hot))\n",
    "            del_b = np.sum(y_hat-y_one_hot)\n",
    "            \n",
    "            self.w = self.w - self.learning_rate*del_w\n",
    "            self.b = self.b - self.learning_rate*del_b\n",
    "            \n",
    "            l = self.crossEntropyLoss(y_hat,y)\n",
    "            self.loss.append(l)\n",
    "#             wandb.log({\"loss\": l, \"epoch\": epoch}) \n",
    "            \n",
    "            \n",
    "            # storing validation accuracy and loss (and other metrics)\n",
    "            self.validate(X_val,y_val)\n",
    "            \n",
    "            # storing training accuracy and loss\n",
    "            y_hat = self.Softmax(X_train@self.w + self.b)\n",
    "            y_pred_train = np.argmax(y_hat,axis=1)\n",
    "            acc = accuracy_score(y_train, y_pred_train)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(y_train, y_pred_train, average='weighted', zero_division=1)\n",
    "            self.train_acc.append(acc)\n",
    "            self.train_prec.append(precision)\n",
    "            self.train_rec.append(recall)\n",
    "            self.train_f1.append(f1)\n",
    "            \n",
    "            \n",
    "#             wandb.log({\"Train Accuracy\": acc, \"epoch\": epoch}) \n",
    "#             wandb.log({\"accuracy\": acc, \"loss\": l})\n",
    "            # if (epoch % (self.num_epoch//10)) ==0:\n",
    "            #     print('Epoch : ',epoch,' Loss : ',l)\n",
    "            \n",
    "        \n",
    "    def validate(self,X_val, y_val):\n",
    "        y_hat = self.Softmax(X_val@self.w + self.b)\n",
    "        y_pred_val = np.argmax(y_hat,axis=1)\n",
    "        # uncomment this to print classification report after each validation\n",
    "        # print(classification_report(self.y_val, y_pred_val,zero_division=1))\n",
    "    \n",
    "        acc = accuracy_score(y_val, y_pred_val)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred_val, average='weighted', zero_division=1)\n",
    "\n",
    "        self.val_acc.append(acc)\n",
    "        self.val_prec.append(precision)\n",
    "        self.val_rec.append(recall)\n",
    "        self.val_f1.append(f1)\n",
    "\n",
    "    def predict(self,X_test,y_test, X_val,y_val):\n",
    "        y_hat = self.Softmax(X_test@self.w + self.b)\n",
    "        self.y_pred = np.argmax(y_hat,axis=1)\n",
    "        cr = classification_report(self.y_pred, y_test, zero_division=1)\n",
    "        # print(cr)\n",
    "        return self.y_pred,y_hat,cr\n",
    "#         print('Test Accuracy : ',round(accuracy_score(self.y_test, self.y_pred),3))\n",
    "#         precision, recall, f1, _ = precision_recall_fscore_support(self.y_test, self.y_pred, average='weighted', zero_division=1)\n",
    "#         print('Precision     : ',round(precision,3))\n",
    "#         print('Recall        : ',round(recall,3))\n",
    "#         print('f1 score      : ',round(f1,3))\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485f8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor:\n",
    "    def __init__(self):\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "        self.confidence_metric = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Add a bias term to the input features\n",
    "        X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "\n",
    "        # Compute the coefficients using the normal equation\n",
    "        self.coefficients = np.linalg.inv(X_train_bias.T.dot(X_train_bias)).dot(X_train_bias.T).dot(y_train)\n",
    "\n",
    "        # Extract the intercept and coefficients\n",
    "        self.intercept = self.coefficients[0]\n",
    "        self.coefficients = self.coefficients[1:]\n",
    "\n",
    "    def predict(self, X,y):\n",
    "        # Add a bias term to the input features\n",
    "        X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "        # Predict the target values\n",
    "        predictions = X_bias.dot(np.insert(self.coefficients, 0, self.intercept))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def compute_confidence_metric(self, X_val, y_val):\n",
    "        # Check for NaN values in input features or target values\n",
    "        if np.isnan(np.sum(X_val)) or np.isnan(np.sum(y_val)):\n",
    "            self.confidence_metric = np.nan\n",
    "        else:\n",
    "            # Use the validation set to compute a performance metric\n",
    "            predictions = self.predict(X_val,y_val)\n",
    "\n",
    "            # Calculate Mean Squared Error as a performance metric\n",
    "            mse = np.mean((y_val - predictions) ** 2)\n",
    "\n",
    "            # Confidence metric is the inverse of MSE (higher confidence for lower MSE)\n",
    "            self.confidence_metric = 1 / (1 + mse)\n",
    "\n",
    "        return self.confidence_metric\n",
    "\n",
    "    def plot_predictions_vs_actual(self,predictions, actual, title=\"Predictions vs Actual\"):\n",
    "        \"\"\"\n",
    "        Plot predictions and actual values on the same graph as a line plot.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions: Numpy array of predicted values\n",
    "        - actual: Numpy array of actual values\n",
    "        - title: Title of the plot (default is \"Predictions vs Actual\")\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(predictions, label='Predictions', marker='o')\n",
    "        plt.plot(actual, label='Actual', marker='x')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Values')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e189966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Classifier:\n",
    "    \n",
    "    def __init__(self,layer_sizes, batch_size, num_epoches, learning_rate, activation_function, optimization):\n",
    "        self.layer_sizes = layer_sizes # number of neurons in each layer\n",
    "        self.num_layers = len(self.layer_sizes) # total number of layers\n",
    "        self.batch_size = batch_size # only for mini-batch gradient descent\n",
    "        self.epoches = num_epoches\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_function = activation_function\n",
    "        self.optimization = optimization\n",
    "        \n",
    "        # initializing weights (needed once only)\n",
    "        self.initialize_weights()\n",
    "        \n",
    "        # validation loss and acc\n",
    "        self.validation_loss = []\n",
    "        self.validation_accuracy = []\n",
    "        \n",
    "        # training loss and acc\n",
    "        self.L = []\n",
    "        self.A = []\n",
    "        \n",
    "        # print('Initialized values successfully!')\n",
    "    \n",
    "    def printParameters(self):\n",
    "        print('-----------------------------------------------')\n",
    "        print('Number of Layers       : ',self.num_layers)\n",
    "        print('Layer sizes            : ',self.layer_sizes)\n",
    "        print('Batch size             : ',self.batch_size)\n",
    "        print('Activation Function    : ',self.activation_function)\n",
    "        print('Optimization Method    : ',self.optimization)\n",
    "        print('Learning Rate          : ',self.learning_rate)\n",
    "        print('Num Epoches            : ',self.epoches)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "    \n",
    "    # loads the dataframe into the class\n",
    "    def loadData(self,df):\n",
    "        self.num_features = df.shape[1]-1\n",
    "        self.num_classes = df.iloc[:, -1].nunique()\n",
    "        # print('Loaded Dataframe!')\n",
    "\n",
    "        \n",
    "    # returns the one-hot encoded version of a given vector\n",
    "    def oneHotEncoding(self, y):\n",
    "        y_encoded = np.zeros((len(y), self.num_classes))\n",
    "        y_encoded[np.arange(len(y)), y] = 1\n",
    "        return y_encoded\n",
    "    \n",
    "    def categorical(self,x):  \n",
    "        # Transform probabilities into categorical predictions row-wise, by simply taking the max probability\n",
    "        categorical = np.argmax(x,axis=1)\n",
    "        return categorical\n",
    "\n",
    "# defining activation functions and their derivatives\n",
    "#--------------------------------------------------------------------------------------------\n",
    "\n",
    "    # SIGMOID\n",
    "    def sigmoid(self,z):\n",
    "        val = 1/(1+np.exp(-z))\n",
    "        return val\n",
    "    def sigmoid_prime(self,h):\n",
    "        # Compute the derivative of sigmoid where h=sigmoid(x)\n",
    "        return h*(1-h)\n",
    "    \n",
    "    # ReLU \n",
    "    def ReLU(self, z):\n",
    "        ret = np.maximum(z, 0)\n",
    "        if ret.shape!=z.shape:\n",
    "            print('whaat??')\n",
    "        return ret\n",
    "    def ReLU_prime(self, h):\n",
    "        return np.where(h >= 0, 1, 0)\n",
    "    \n",
    "    # tanh\n",
    "    def hyperbolic_tan(self, z):\n",
    "#         val = (2/(np.exp(-2*z)+1)) -1\n",
    "        val = expit(2 * z) - 1\n",
    "\n",
    "        return val\n",
    "    def hyperbolic_tan_prime(self, h):\n",
    "        return 1-h**2\n",
    "    \n",
    "    # linear (can change slope and intercept)\n",
    "    def linear(self, z):\n",
    "        return z\n",
    "    def linear_prime(self,h):\n",
    "        return 1\n",
    "    \n",
    "#-----------------activation function for last layer (output) -------------------------------\n",
    "\n",
    "    def softmax(self,z):\n",
    "        ar = np.exp(z-np.max(z))\n",
    "        return ar/ar.sum(axis=1,keepdims=True)\n",
    "#         exp = np.exp(z - np.max(z))\n",
    "#         for i in range(len(z)):\n",
    "#             exp[i]/=np.sum(exp[i])\n",
    "#         return exp\n",
    "    \n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def activation(self,z):\n",
    "        if z.any()>=1e2:\n",
    "            print('z = ',z)\n",
    "        if self.activation_function=='sigmoid':\n",
    "            return self.sigmoid(z)\n",
    "        elif self.activation_function=='ReLU':\n",
    "            return self.ReLU(z)\n",
    "        elif self.activation_function=='tanh':\n",
    "            return self.hyperbolic_tan(z)\n",
    "        elif self.activation_function=='linear':\n",
    "            return self.linear(z)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation function\")\n",
    "\n",
    "    def activation_prime(self,h):\n",
    "        if self.activation_function=='sigmoid':\n",
    "            return self.sigmoid_prime(h)\n",
    "        elif self.activation_function=='ReLU':\n",
    "            return self.ReLU_prime(h)\n",
    "        elif self.activation_function=='tanh':\n",
    "            return self.hyperbolic_tan_prime(h)\n",
    "        elif self.activation_function=='linear':\n",
    "            return self.linear_prime(h)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation function\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        epsilon = 1e-7\n",
    "        y_hat = y_hat + epsilon\n",
    "        # adding small value to avoid underflow\n",
    "        # Compute the loss along the rows, averaging along the number of samples\n",
    "        return ((-np.log(y_hat))*y).mean()\n",
    "    \n",
    "    def accuracy(self, y_hat, y):  \n",
    "        # Compute the accuracy along the rows, averaging along the number of samples\n",
    "        return np.sum(y_hat==(y))/len(y_hat)\n",
    "\n",
    "\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        self.weights = []\n",
    "        #weights[i] is a m,n matrix giving weight connecting the i and i+1th layer\n",
    "        \n",
    "        min_weight = -1\n",
    "        max_weight = 1\n",
    "        for i in range(self.num_layers-1):\n",
    "            sz = [self.layer_sizes[i],self.layer_sizes[i+1]]\n",
    "            wt = np.random.uniform(min_weight,max_weight,size=sz)\n",
    "            self.weights.append(wt)\n",
    "        \n",
    "#         self.weights = np.asarray(self.weights)\n",
    "        \n",
    "    def initialize_layers(self,batch_size):\n",
    "        self.hidden_layers = [np.ones((batch_size,layer_size)) for layer_size in layer_sizes]\n",
    "        \n",
    "    # takes in a batch of data (num_samples x num_features) and applies feed-forward on it\n",
    "    def feedforward(self, batch):\n",
    "        current_layer = batch\n",
    "        self.hidden_layers[0] = batch\n",
    "        \n",
    "        # next_layer = activation(weights . current_leyer) and storing while feed-forward\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            current_layer = self.activation(np.dot(current_layer,weights))\n",
    "            self.hidden_layers[i+1] = current_layer\n",
    "            \n",
    "        # softmax of the last layer is the output layer\n",
    "        self.output_layer = self.softmax(self.hidden_layers[-1])\n",
    "        \n",
    "    # goes from the last layer to the 1st layer updating weights according to GD\n",
    "    def backpropogation(self, y):\n",
    "        y = self.oneHotEncoding(y)\n",
    "        # evaluating the last layer error : del\n",
    "        del_t = -(y - self.output_layer)*self.activation_prime(self.hidden_layers[-1])\n",
    "        \n",
    "        for i in range(1,self.num_layers):\n",
    "            # calculating the gradient of weights and applying gradient-descent\n",
    "            dJ_dW = np.dot(self.hidden_layers[-i-1].T,del_t)/self.batch_size\n",
    "            self.weights[-i] = self.weights[-i] - self.learning_rate * dJ_dW\n",
    "            # updating the error for the next layer \n",
    "            del_t = np.dot(del_t,self.weights[-i].T)*self.activation_prime(self.hidden_layers[-i-1])\n",
    "                    \n",
    "    def validate(self,X_val,y_val):\n",
    "        n,m = X_val.shape[0:2]\n",
    "        self.initialize_layers(batch_size=n)\n",
    "        self.feedforward(X_val)\n",
    "        val_loss = self.loss(self.output_layer,self.oneHotEncoding(y_val))\n",
    "        val_acc = self.accuracy(self.categorical(self.output_layer),y_val)\n",
    "        self.validation_loss.append(val_loss)\n",
    "        self.validation_accuracy.append(val_acc)\n",
    "        \n",
    "    def mini_batch_GD(self,X_train,y_train):\n",
    "            self.initialize_layers(self.batch_size)\n",
    "            loss_sum = 0\n",
    "            accuracy_sum = 0\n",
    "            \n",
    "            # calculating the total number of batches (acc to batch sizee)\n",
    "            num_batches = X_train.shape[0]/self.batch_size\n",
    "            ind = np.random.permutation(X_train.shape[0])\n",
    "            \n",
    "            # splitting the X_train, Y_train into batches\n",
    "            X_batches = np.array_split(X_train[ind], num_batches)\n",
    "            Y_batches = np.array_split(y_train[ind], num_batches)\n",
    "            data_batches = zip(X_batches,Y_batches)\n",
    "            \n",
    "            # performing feed-forward -> saving training loss and accuracy -> back-propogation\n",
    "            for data_x, data_y in data_batches:\n",
    "                self.feedforward(data_x)\n",
    "                loss_sum = loss_sum + self.loss(self.output_layer,self.oneHotEncoding(data_y))\n",
    "                accuracy_sum = accuracy_sum + self.accuracy(self.categorical(self.output_layer),data_y)\n",
    "                self.backpropogation(data_y)\n",
    "            \n",
    "            loss_train = loss_sum/num_batches\n",
    "            acc_train = accuracy_sum/num_batches\n",
    "            \n",
    "            return loss_train, acc_train\n",
    "        \n",
    "    def batch_GD(self,X_train,y_train):\n",
    "        self.batch_size = X_train.shape[0]\n",
    "        loss_train, acc_train = self.mini_batch_GD(X_train,y_train)\n",
    "        return loss_train, acc_train\n",
    "        \n",
    "    def SGD(self,X_train,y_train):\n",
    "        self.batch_size = 1\n",
    "        loss_train, acc_train = self.mini_batch_GD(X_train,y_train)\n",
    "        return loss_train, acc_train\n",
    "    \n",
    "    # general method for optimization (batch/mini-batch/SGD)\n",
    "    def optimize(self,X_train,y_train):\n",
    "        if self.optimization=='mini-batch':\n",
    "            return self.mini_batch_GD(X_train,y_train)\n",
    "        elif self.optimization=='batch':\n",
    "            return self.batch_GD(X_train,y_train)\n",
    "        elif self.optimization=='SGD':\n",
    "            return self.SGD(X_train,y_train)\n",
    "    \n",
    "    def fit(self,X_train,y_train,X_val,y_val):\n",
    "        for epoch in range(self.epoches):\n",
    "            # Gradient Descent\n",
    "            loss_train, acc_train = self.optimize(X_train,y_train)\n",
    "            \n",
    "            # calculating accuracy and loss for current epoch and saving them\n",
    "            self.L.append(loss_train)\n",
    "            self.A.append(acc_train)\n",
    "            \n",
    "            # testing the current model on validation set and saving the loss and accuracy\n",
    "            self.validate(X_val,y_val)\n",
    "            # if (epoch%(self.epoches//10)==0):\n",
    "                # print('Epoch : ',epoch+1,' loss : ',loss_train.round(3),' acc : ',acc_train.round(3))\n",
    "#             wandb.log({\"acc\": acc_train.round(3), \"loss\": loss_train.round(3)})\n",
    "            \n",
    "            \n",
    "    def predict(self,X_test,y_test,X_val,y_val):\n",
    "        self.feedforward(X_test)\n",
    "        cr = classification_report(self.categorical(self.output_layer),y_test,zero_division=1)\n",
    "        self.predictions = self.categorical(self.output_layer)\n",
    "        # print(self.predictions)\n",
    "        # print(self.output_layer)\n",
    "        return self.predictions,self.output_layer,cr\n",
    "        # return self.predictions,cr\n",
    "        # print('------------------------------------------------------\\n',cr,'\\n------------------------------------------------------\\n')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3023b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Regressor:\n",
    "    \n",
    "    def __init__(self,layer_sizes, batch_size, num_epoches, learning_rate, activation_function, optimization):\n",
    "        self.layer_sizes = layer_sizes # number of neurons in each layer\n",
    "        self.num_layers = len(self.layer_sizes) # total number of layers\n",
    "        self.batch_size = batch_size # only for mini-batch gradient descent\n",
    "        self.epoches = num_epoches\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_function = activation_function\n",
    "        self.optimization = optimization\n",
    "        \n",
    "        # initializing weights (needed once only)\n",
    "        self.initialize_weights()\n",
    "        \n",
    "        # validation loss and acc\n",
    "        self.validation_loss = []\n",
    "        self.validation_accuracy = []\n",
    "        \n",
    "        # training loss and acc\n",
    "        self.L = []\n",
    "        self.A = []\n",
    "        \n",
    "        # print('Initialized values successfully!')\n",
    "    \n",
    "    def printParameters(self):\n",
    "        print('-----------------------------------------------')\n",
    "        print('Number of Layers          : ',self.num_layers)\n",
    "        print('Layer sizes               : ',self.layer_sizes)\n",
    "        print('Batch size(if mini-batch) : ',self.batch_size)\n",
    "        print('Activation Function       : ',self.activation_function)\n",
    "        print('Optimization Method       : ',self.optimization)\n",
    "        print('Learning Rate             : ',self.learning_rate)\n",
    "        print('Num Epoches               : ',self.epoches)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "    \n",
    "    # loads the dataframe into the class\n",
    "    def loadData(self,df):\n",
    "        self.num_features = df.shape[1]-1\n",
    "        self.num_classes = df.iloc[:, -1].nunique()\n",
    "        self.df = df\n",
    "        # print('Loaded Dataframe!')\n",
    "\n",
    "    # splits the dataframe loaded into train, validation and test sets\n",
    "    # def splitData(self,train_fraction,normalize):\n",
    "    #     X = df.iloc[:, :-1].values\n",
    "        \n",
    "    #     # Replace NaN values with the constant k\n",
    "    #     nan_mask = np.isnan(X)\n",
    "    #     k = 0\n",
    "    #     X[nan_mask] = k\n",
    "\n",
    "    #     if normalize:\n",
    "    #         mean = np.mean(X, axis=0)\n",
    "    #         std = np.std(X, axis=0)\n",
    "    #         X = (X - mean) / std\n",
    "    #     y = df.iloc[:, -1].values\n",
    "        \n",
    "\n",
    "    #     self.X_train, X_temp, self.y_train, y_temp = train_test_split(X, y, test_size=1-train_fraction, random_state=42)\n",
    "    #     self.X_val, self.X_test, self.y_val, self.y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    #     print('Splitted data into train, test and val sets!')\n",
    "        \n",
    "        \n",
    "    # returns the one-hot encoded version of a given vector\n",
    "    def oneHotEncoding(self, y):\n",
    "        return y\n",
    "        y_encoded = np.zeros((len(y), self.num_classes))\n",
    "        y_encoded[np.arange(len(y)), y] = 1\n",
    "        return y_encoded\n",
    "    \n",
    "    def categorical(self,x):  \n",
    "        return x\n",
    "    \n",
    "        # Transform probabilities into categorical predictions row-wise, by simply taking the max probability\n",
    "        categorical = np.argmax(x,axis=1)\n",
    "        return categorical\n",
    "\n",
    "# defining activation functions and their derivatives\n",
    "#--------------------------------------------------------------------------------------------\n",
    "\n",
    "    # SIGMOID\n",
    "    def sigmoid(self,z):\n",
    "        val = 1/(1+np.exp(-z))\n",
    "        return val\n",
    "    \n",
    "    def sigmoid_prime(self,h):\n",
    "        # Compute the derivative of sigmoid where h=sigmoid(x)\n",
    "        return h*(1-h)\n",
    "    \n",
    "    # ReLU \n",
    "    def ReLU(self, z):\n",
    "        ret = np.maximum(z, 0)\n",
    "        if ret.shape!=z.shape:\n",
    "            print('whaat??')\n",
    "        return ret\n",
    "    def ReLU_prime(self, h):\n",
    "        return np.where(h >= 0, 1, 0)\n",
    "    \n",
    "    # tanh\n",
    "    def hyperbolic_tan(self, z):\n",
    "        val = (2/(np.exp(-2*z)+1)) -1\n",
    "        val = expit(2 * z) - 1\n",
    "        return val\n",
    "    \n",
    "    def hyperbolic_tan_prime(self, h):\n",
    "        return 1-h**2\n",
    "    \n",
    "    # linear (can change slope and intercept)\n",
    "    def linear(self, z):\n",
    "        return z\n",
    "    def linear_prime(self,h):\n",
    "        return 1\n",
    "    \n",
    "#-----------------activation function for last layer (output) -------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def activation(self,z):\n",
    "        if self.activation_function=='sigmoid':\n",
    "            return self.sigmoid(z)\n",
    "        elif self.activation_function=='ReLU':\n",
    "            return self.ReLU(z)\n",
    "        elif self.activation_function=='tanh':\n",
    "            return self.hyperbolic_tan(z)\n",
    "        elif self.activation_function=='linear':\n",
    "            return self.linear(z)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation function\")\n",
    "\n",
    "    def activation_prime(self,h):\n",
    "        if self.activation_function=='sigmoid':\n",
    "            return self.sigmoid_prime(h)\n",
    "        elif self.activation_function=='ReLU':\n",
    "            return self.ReLU_prime(h)\n",
    "        elif self.activation_function=='tanh':\n",
    "            return self.hyperbolic_tan_prime(h)\n",
    "        elif self.activation_function=='linear':\n",
    "            return self.linear_prime(h)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation function\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        y = y.reshape(-1,1)\n",
    "        return np.sum((y_hat-y)**2)\n",
    "    \n",
    "    def accuracy(self, y_hat, y):  \n",
    "        nan_mask = np.isnan(y)\n",
    "        k = 0\n",
    "        y[nan_mask] = k\n",
    "        nan_mask = np.isnan(y_hat)\n",
    "        k = 0\n",
    "        y_hat[nan_mask] = k\n",
    "        \n",
    "\n",
    "        mse = mean_squared_error(y_hat, y)\n",
    "        return mse\n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights = []\n",
    "        #weights[i] is a m,n matrix giving weight connecting the i and i+1th layer\n",
    "        \n",
    "        mean_weight = 0  # Mean of the normal distribution\n",
    "        std_dev_weight = 1  # Standard deviation of the normal distribution\n",
    "        min_weight = -1\n",
    "        max_weight = 1\n",
    "        for i in range(self.num_layers-1):\n",
    "            sz = [self.layer_sizes[i],self.layer_sizes[i+1]]\n",
    "            wt = np.random.normal(mean_weight, std_dev_weight, size=sz)\n",
    "\n",
    "            self.weights.append(wt)\n",
    "        \n",
    "    def initialize_layers(self,batch_size):\n",
    "        self.hidden_layers = [np.ones((batch_size,layer_size)) for layer_size in layer_sizes]\n",
    "        \n",
    "    # takes in a batch of data (num_samples x num_features) and applies feed-forward on it\n",
    "    def feedforward(self, batch):\n",
    "        current_layer = batch\n",
    "        self.hidden_layers[0] = batch\n",
    "\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            current_layer = self.activation(np.dot(current_layer,weights))\n",
    "\n",
    "            self.hidden_layers[i+1] = current_layer\n",
    "            \n",
    "        # last layer is the output layer\n",
    "        self.output_layer = (self.hidden_layers[-1])\n",
    "        \n",
    "    # goes from the last layer to the 1st layer updating weights according to GD\n",
    "    def backpropogation(self, y):\n",
    "        # evaluating the last layer error : del\n",
    "        y = y.reshape(-1, 1)\n",
    "        del_t = -(y - self.output_layer)\n",
    "        for i in range(1,self.num_layers):\n",
    "            # calculating the gradient of weights and applying gradient-descent\n",
    "            dJ_dW = np.dot(self.hidden_layers[-i-1].T,del_t)/self.batch_size\n",
    "            self.weights[-i] = self.weights[-i] - self.learning_rate * dJ_dW\n",
    "            # updating the error for the next layer \n",
    "            del_t = np.dot(del_t,self.weights[-i].T)*self.activation_prime(self.hidden_layers[-i-1])\n",
    "                    \n",
    "    def validate(self,X_val,y_val):\n",
    "        n,m = X_val.shape[0:2]\n",
    "        self.initialize_layers(batch_size=n)\n",
    "        self.feedforward(X_val)\n",
    "        val_loss = self.loss(self.output_layer,y_val)\n",
    "        val_acc = self.accuracy(self.categorical(self.output_layer),y_val)\n",
    "        self.validation_loss.append(val_loss)\n",
    "        self.validation_accuracy.append(val_acc)\n",
    "        \n",
    "    def mini_batch_GD(self,X_train,y_train,X_val,y_val):\n",
    "            self.initialize_layers(self.batch_size)\n",
    "            loss_sum = 0\n",
    "            accuracy_sum = 0\n",
    "            \n",
    "            # calculating the total number of batches (acc to batch sizee)\n",
    "            num_batches = X_train.shape[0]/self.batch_size\n",
    "            ind = np.random.permutation(X_train.shape[0])\n",
    "            \n",
    "            \n",
    "            # splitting the X_train, Y_train into batches\n",
    "            X_batches = np.array_split(X_train[ind], num_batches)\n",
    "            Y_batches = np.array_split(y_train[ind], num_batches)\n",
    "            data_batches = zip(X_batches,Y_batches)\n",
    "            \n",
    "            # performing feed-forward -> saving training loss and accuracy -> back-propogation\n",
    "            for data_x, data_y in data_batches:\n",
    "                self.feedforward(data_x)\n",
    "\n",
    "                loss_sum = loss_sum + self.loss(self.output_layer,data_y)\n",
    "                accuracy_sum = accuracy_sum + self.accuracy(self.categorical(self.output_layer),data_y)\n",
    "                self.backpropogation(data_y)\n",
    "            \n",
    "            loss_train = loss_sum/num_batches\n",
    "            acc_train = accuracy_sum/num_batches\n",
    "            \n",
    "            return loss_train, acc_train\n",
    "        \n",
    "    def batch_GD(self,X_train,y_train,X_val,y_val):\n",
    "        self.batch_size = X_train.shape[0]\n",
    "        loss_train, acc_train = self.mini_batch_GD(X_train,y_train,X_val,y_val)\n",
    "        return loss_train, acc_train\n",
    "        \n",
    "    def SGD(self,X_train,y_train,X_val,y_val):\n",
    "        self.batch_size = 1\n",
    "        loss_train, acc_train = self.mini_batch_GD(X_train,y_train,X_val,y_val)\n",
    "        return loss_train, acc_train\n",
    "    \n",
    "    # general method for optimization (batch/mini-batch/SGD)\n",
    "    def optimize(self,X_train,y_train,X_val,y_val):\n",
    "        if self.optimization=='mini-batch':\n",
    "            return self.mini_batch_GD(X_train,y_train,X_val,y_val)\n",
    "        elif self.optimization=='batch':\n",
    "            return self.batch_GD(X_train,y_train,X_val,y_val)\n",
    "        elif self.optimization=='SGD':\n",
    "            return self.SGD(X_train,y_train,X_val,y_val)\n",
    "    \n",
    "    def fit(self,X_train,y_train,X_val,y_val):\n",
    "        for epoch in range(self.epoches):\n",
    "            # Gradient Descent\n",
    "            loss_train, acc_train = self.optimize(X_train,y_train,X_val,y_val)\n",
    "            \n",
    "            # calculating accuracy and loss for current epoch and saving them\n",
    "            self.L.append(loss_train)\n",
    "            self.A.append(acc_train)\n",
    "            \n",
    "            # testing the current model on validation set and saving the loss and accuracy\n",
    "            self.validate(X_val,y_val)\n",
    "            # if (epoch%(0.1*self.epoches)==0):\n",
    "            #     print('Epoch : ',epoch+1,' loss : ',loss_train.round(3),' acc : ',acc_train.round(3))\n",
    "\n",
    "\n",
    "    def compute_confidence_metric(self,X_val,y_val):\n",
    "        # y_val = y_val.reshape(-1,1)\n",
    "        self.feedforward(X_val)\n",
    "        predictions = self.output_layer\n",
    "        mse = np.mean((y_val - predictions) ** 2)\n",
    "        # Confidence metric is the inverse of MSE (higher confidence for lower MSE)\n",
    "        self.confidence_metric = 1 / (1 + mse)\n",
    "        return self.confidence_metric\n",
    "\n",
    "            \n",
    "    def predict(self,X_test,y_test):\n",
    "        # y_test = y_test.reshape(-1,1)\n",
    "        self.feedforward(X_test)\n",
    "        \n",
    "        # plt.plot(self.output_layer,label='Predicted')\n",
    "        # plt.plot(self.y_test,label='Ground Truth')\n",
    "        # plt.title('Predicted Values vs Ground Truth')\n",
    "        # plt.grid()\n",
    "        # plt.xlabel('Sample')\n",
    "        # plt.ylabel('Value')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "        \n",
    "        output_layer = self.output_layer\n",
    "        return output_layer\n",
    "        \n",
    "        # print('--------------------------------------------------------------------------------')\n",
    "        \n",
    "        # # Calculate Mean Squared Error (MSE)\n",
    "        # mse = mean_squared_error(y_test, output_layer)\n",
    "        # print(f'Mean Squared Error (MSE)                 : {mse:.2f}')\n",
    "\n",
    "        # # Calculate Root Mean Squared Error (RMSE)\n",
    "        # rmse = np.sqrt(mse)\n",
    "        # print(f'Root Mean Squared Error (RMSE)           : {rmse:.2f}')\n",
    "\n",
    "        # # Calculate R-squared (Coefficient of Determination)\n",
    "        # r_squared = r2_score(y_test, output_layer)\n",
    "        # print(f'R-squared (Coefficient of Determination) : {r_squared:.2f}')\n",
    "        \n",
    "        # print('--------------------------------------------------------------------------------')\n",
    "        \n",
    "    def plot_predictions_vs_actual(self,predictions, actual, title=\"Predictions vs Actual\"):\n",
    "        \"\"\"\n",
    "        Plot predictions and actual values on the same graph as a line plot.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions: Numpy array of predicted values\n",
    "        - actual: Numpy array of actual values\n",
    "        - title: Title of the plot (default is \"Predictions vs Actual\")\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(predictions, label='Predictions', marker='o')\n",
    "        plt.plot(actual, label='Actual', marker='x')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Values')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f64175",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the Decision Tree Classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - **kwargs: Any additional arguments accepted by DecisionTreeClassifier\n",
    "        \"\"\"\n",
    "        self.model = DecisionTreeClassifier(**kwargs)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the Decision Tree Classifier to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training features\n",
    "        - y_train: Training labels\n",
    "        \"\"\"\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def validate(self,X_val, y_val):\n",
    "        predictions = self.model.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, predictions)\n",
    "        return accuracy\n",
    "\n",
    "    \n",
    "    def predict(self, X_test,y_test,X_val, y_val):\n",
    "        \"\"\"\n",
    "        Predict labels, soft assignments, and provide the classification report.\n",
    "\n",
    "        Parameters:\n",
    "        - X_test: Test features\n",
    "\n",
    "        Returns:\n",
    "        - predictions: Predicted labels\n",
    "        - soft_assignments: Soft assignments for each prediction\n",
    "        - report: Classification report\n",
    "        \"\"\"\n",
    "        value = self.validate(X_val, y_val)\n",
    "        predictions = self.model.predict(X_test)\n",
    "        soft_assignments = self.model.predict_proba(X_test)\n",
    "        report = classification_report(predictions,y_test,zero_division=1)\n",
    "\n",
    "        return predictions, value*soft_assignments, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5483d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeRegressor:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the Decision Tree Regressor.\n",
    "\n",
    "        Parameters:\n",
    "        - **kwargs: Any additional arguments accepted by DecisionTreeRegressor\n",
    "        \"\"\"\n",
    "        self.model = DecisionTreeRegressor(**kwargs)\n",
    "        self.confidence_metric = None\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Fit the Decision Tree Regressor to the training data and compute the confidence metric.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training features\n",
    "        - y_train: Training labels\n",
    "        - X_val: Validation features\n",
    "        - y_val: Validation labels\n",
    "        \"\"\"\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on validation data for computing confidence metric\n",
    "        predictions_val = self.model.predict(X_val)\n",
    "        mse_val = mean_squared_error(y_val, predictions_val)\n",
    "\n",
    "        # Confidence metric is the inverse of MSE on validation data (higher confidence for lower MSE)\n",
    "        self.confidence_metric = 1 / (1 + mse_val)\n",
    "\n",
    "    def compute_confidence_metric(self,X_val,y_val):\n",
    "        confidence_metric = self.confidence_metric\n",
    "        return confidence_metric\n",
    "\n",
    "\n",
    "    def predict(self, X_test,y_test):\n",
    "        \"\"\"\n",
    "        Predict values and provide a confidence metric.\n",
    "\n",
    "        Parameters:\n",
    "        - X_test: Test features\n",
    "\n",
    "        Returns:\n",
    "        - predictions: Predicted values\n",
    "        - confidence_metric: Confidence metric\n",
    "        \"\"\"\n",
    "        predictions = self.model.predict(X_test)\n",
    "        return predictions\n",
    "    \n",
    "    def plot_predictions_vs_actual(self,predictions, actual, title=\"Predictions vs Actual\"):\n",
    "        \"\"\"\n",
    "        Plot predictions and actual values on the same graph as a line plot.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions: Numpy array of predicted values\n",
    "        - actual: Numpy array of actual values\n",
    "        - title: Title of the plot (default is \"Predictions vs Actual\")\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(predictions, label='Predictions', marker='o')\n",
    "        plt.plot(actual, label='Actual', marker='x')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Values')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f851a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915f4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
